{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu8N7G7GEd0L"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 001 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 0.1.001 | 02/04/2023 | Royi Avital | Fixed a typo in question `0.1.`                                    |\n",
        "| 0.1.000 | 12/03/2023 | Royi Avital | First version                                                      |\n",
        "|         |            |             |                                                                    |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eW3omZBFEd0O"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/0004EstimationNonParametric.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nsNanS30Ed0O"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wbk2nFYEd0P"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Answer all questions within the Jupyter Notebook.\n",
        " - Open questions are in part I of the exercise.\n",
        " - Coding based questions are in the subsequent notebooks.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for question.\n",
        " - Good Luck!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B3NplsAyOXOn"
      },
      "source": [
        "## 0. Linear Algebra\n",
        "\n",
        "A matrix $ P $ is called an orthogonal projection operator if, and only if it is idempotent and symmetric.\n",
        "\n",
        "**Remark**: Idempotent matrix means $ \\forall n \\in \\mathcal{N} \\; {P}^{n} = P $.\n",
        "\n",
        "### 0.1. Question\n",
        "\n",
        "Let $A \\in \\mathbb{R}^{m \\times n}$ where $ m \\geq n $ and $ \\operatorname{Rank} \\left( A \\right) = n $.  \n",
        "Given the linear least squares problem:\n",
        "\n",
        "$$ \\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2} $$\n",
        "\n",
        "With the solution in the form $\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$, show that $P = A R$ is an orthogonal projection operator.\n",
        "\n",
        "**Hints**\n",
        "\n",
        "1. Derive the solution to the Least Squares above in the form of $ \\hat{\\boldsymbol{x}} = R \\boldsymbol{y} $.\n",
        "2. Show the $ P $ matrix is symmetric.\n",
        "3. Show the $ P $ matrix is idempotent.\n",
        "4. Conclude the matrix is an orthogonal projection operator.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The matrix $P$ is the Orthogonal Projection onto the range (Columns space) of $ A $."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2pnx4EdwPC"
      },
      "source": [
        "### 0.1. Solution\n",
        "\n",
        "The solution to the linear Least Squares problem $\\arg \\min_{\\boldsymbol{x}} \\frac{1}{2} {\\left\\| A \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{2}^{2}$ can be derived by setting the $\\nabla f\\left(\\boldsymbol{x}\\right)=0$ and solving for $\\boldsymbol{x}$.\n",
        "\n",
        "The gradient with respect to $\\boldsymbol{x}$ is given by:\n",
        "\n",
        "$$\\nabla f\\left(\\boldsymbol{x}\\right) = A^T (A \\boldsymbol{x} - \\boldsymbol{y}) $$\n",
        "\n",
        "Setting the gradient to zero and solving for $\\boldsymbol{x}$, we get:\n",
        "\n",
        "$$ A^T (A \\boldsymbol{x} - \\boldsymbol{y}) = 0 $$\n",
        "\n",
        "$$ A^T A \\boldsymbol{x} = A^T \\boldsymbol{y} $$\n",
        "\n",
        "$$ \\hat{\\boldsymbol{x}} = (A^T A)^{-1} A^T \\boldsymbol{y} $$\n",
        "\n",
        "Thus, the solution to the linear Least Squares problem can be written in the form $\\hat{\\boldsymbol{x}} = R \\boldsymbol{y}$ where $R = (A^T A)^{-1} A^T$.\n",
        "\n",
        "Now, let $P = A R$. Then $P = A (A^T A)^{-1} A^T$. \n",
        "\n",
        "It can be shown that $P$ is an orthogonal projection operator by verifying that $P^2 = P$ (If this is true, it can be proven true for $\\forall n$ by induction) and $P^T = P$.\n",
        "\n",
        "First, we prove that P is idempotent: \n",
        "\n",
        "$ P^2 = (A (A^T A)^{-1} A^T)(A (A^T A)^{-1} A^T) = A (A^T A)^{-1} (A^T A) (A^T A)^{-1} A^T = A (A^T A)^{-1} A^T = P $\n",
        "\n",
        "Thus, $P^2 = P$.\n",
        "\n",
        "Next, we prove that P is symmetric:\n",
        "\n",
        "$P^{T}=\\left(A(A^{T}A)^{-1}A^{T}\\right)^{T}=A\\left((A^{T}A)^{-1}\\right)^{T}A^{T}=A\\left((A^{T}A)^{T}\\right)^{-1}A^{T}=A(A^{T}A)^{-1}A^{T}=P$\n",
        "\n",
        "Thus, $P$ is an orthogonal projection operator.\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "czWSZsWjEd0P"
      },
      "source": [
        "## 1. Convexity\n",
        "\n",
        "**Convex Set**  \n",
        "\n",
        "Let:\n",
        "\n",
        "$$ \\mathbb{R}_{\\geq 0}^{d} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $$\n",
        "\n",
        "Where $\\boldsymbol{x} = \\begin{bmatrix} {x}_{1} \\\\ {x}_{2} \\\\ \\vdots \\\\ {x}_{d} \\end{bmatrix}$\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Prove or disprove that $\\mathbb{R}_{\\geq 0}^{d}$ is convex."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eS67Jlv-Ed0P"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "Let $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}_{\\geq0}^{d} $\n",
        "\n",
        "Define $\\boldsymbol{z}=:\\alpha\\boldsymbol{x}+(1-\\alpha)\\boldsymbol{y},s.t. \\alpha\\in[0,1]$\n",
        "\n",
        "$\\min\\limits_{i} {x}_{i}, \\min\\limits_{i} y_{i}\\geq0\\Rightarrow z_{i}\\geq0 , \\forall i$\n",
        "\n",
        "$\\Rightarrow\\min\\limits_{i} z_{i} \\geq 0 $\n",
        "\n",
        "\n",
        "$\\Rightarrow \\boldsymbol{z}\\in\\mathbb{R}_{\\geq0}^{d}$\n",
        "\n",
        "$\\mathbb{R}_{\\geq0}^{d}$ is a convex by definition.\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cdrCRhVHEd0Q"
      },
      "source": [
        "**Convex Combination** \n",
        "\n",
        "Let $\\mathcal{C} \\subseteq \\mathbb{R}^{d} $ be a convex set and consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\} _{i=1}^{N}$.\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Prove that for any $N \\in \\mathbb{N}$: \n",
        "\n",
        "$$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
        "\n",
        "Where $\\alpha_{i}$ are such that: \n",
        "\n",
        " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
        " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The properties of ${\\alpha}_{i}$ above means it is sampled from the Unit Probability Simplex.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CArmL4VCEd0Q"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "Solution by induction on $N$:\n",
        "\n",
        "**Base case**: $N = 1:$\n",
        "\n",
        "Only one $\\boldsymbol{x}$: $\\alpha \\boldsymbol{x} \\in \\mathcal{C}$\n",
        "\n",
        "**Induction step:** \n",
        "\n",
        "Assume $N-1$ combinations are also $\\in \\mathcal{C}$, let's prove: $ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $\n",
        "\n",
        "By the inductive hypothesis:\n",
        " \n",
        "$\\boldsymbol{y} = \\sum_{j=1}^{N-1}\\frac{\\alpha_{j}}{\\sum_{i=1}^{N-1}\\alpha_{i}}\\boldsymbol{x_{j}} \\in \\mathcal{C}$ \n",
        "(assuming $\\sum_{i=1}^{N-1}\\alpha_{i}\\neq 0$; Otherwise, $\\alpha_{N}$ is the only nonzero coefficient - solved by Base case)\n",
        "\n",
        "We can rewrite the N convex combinations:\n",
        "\n",
        "$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} = \\sum_{i=1}^{N-1}\\alpha_{i}\\boldsymbol{y} + \\alpha_{N}\\boldsymbol{x}$, Or:\n",
        "\n",
        "$(1-\\alpha_{N})\\boldsymbol{y} + \\alpha_{N}\\boldsymbol{x}$ which by definition is $\\in \\mathcal{C} $\n",
        "\n",
        "Thus, by induction, convex combinations of all size $\\in \\mathcal{C}$.\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w4LW4L4SEd0Q"
      },
      "source": [
        "Let $\\mathcal{C}\\subset\\mathbb{R}^{2}$ be a convex set.  \n",
        "Consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ such that $\\boldsymbol{x}_{i} \\neq \\boldsymbol{x}_{j}$ for all $i \\neq j$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AHcDl4XbUOiZ"
      },
      "source": [
        "### 1.3. Question\n",
        "\n",
        "Prove or disprove the following assertion:\n",
        "\n",
        "Necessarily, any point $\\boldsymbol{y} \\in \\mathcal{C}$ can be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EpWHs7UOiZ"
      },
      "source": [
        "### 1.3. Solution\n",
        "\n",
        "Disprove by counterexample:\n",
        "\n",
        "Let, $ \\mathcal{C} =  \\mathbb{R}_{\\geq 0}^{2} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $ - $ \\mathcal{C}\\subset\\mathbb{R}^{2}$ and proven to be convex set in question 1.1.\n",
        "\n",
        "\n",
        "Let, $ \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} | {x}_{i}= \\begin{bmatrix} 1 \\\\ i \\\\ \\end{bmatrix}\\right\\}_{i=1}^{10} , {y} = \\begin{bmatrix} 12 \\\\ 1 \\\\ \\end{bmatrix}$\n",
        "\n",
        "$ \\sum_{i = 1}^{10} {\\alpha}_{i} \\boldsymbol{x}_{i} \\neq y, \\forall\\alpha \\in [0,1] $\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4agjp2tqEd0R"
      },
      "source": [
        "## 2. The Gradient\n",
        "\n",
        "**Remark**: Assume all functions in this section are differentiable.\n",
        "\n",
        "\n",
        "**Directional Derivative**\n",
        "\n",
        "Let $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ and let $\\boldsymbol{x}_{0} \\in \\mathbb{R}^{d}$. \n",
        "\n",
        "### 2.1. Question\n",
        "\n",
        "Prove that:\n",
        "\n",
        "$$ \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HVvrm9mZUOia"
      },
      "source": [
        "### 2.1. Solution\n",
        "Assuming that for all $\\boldsymbol{h} \\in \\mathbb{R}^d$ we have:\n",
        "\n",
        "$$\\nabla f \\left( \\boldsymbol{x}{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_0, \\boldsymbol{h} \\right\\rangle$$\n",
        "\n",
        "we want to show that $\\boldsymbol{g}_0 = \\nabla f \\left( \\boldsymbol{x}_0 \\right)$.\n",
        "\n",
        "To prove this, we will show that the components of a vector $\\boldsymbol{g}_0$ are the partial derivatives of $f$ at a point $\\boldsymbol{x}_0$.\n",
        "\n",
        "Consider  $\\boldsymbol{h}= \\boldsymbol{e}_i$ where $\\boldsymbol{e}_i$ is the $i$-th unit vector in $\\mathbb{R}^d$. \n",
        "\n",
        "By the assumption we have:\n",
        "\n",
        "$$\\nabla f(\\boldsymbol{x}_0) [\\boldsymbol{e}_i] = \\left\\langle \\boldsymbol{g}_0, \\boldsymbol{e}i \\right\\rangle = \\boldsymbol{g}_{0_{i}}$$\n",
        "\n",
        "By the definition of directional derivative, we have:\n",
        "\n",
        "$$\\nabla f\\left(\\boldsymbol{x}_{0}\\right)\\left[\\boldsymbol{\\boldsymbol{e}_{i}}\\right]=\\lim_{t\\to0}\\frac{f(\\boldsymbol{x}_{0}+t\\boldsymbol{e}_{i})-f(\\boldsymbol{x}_{0})}{t}=\\frac{\\partial}{\\partial x_{i}}f\\left(\\boldsymbol{x}_{0}\\right)$$\n",
        "\n",
        "Thus, \n",
        "\n",
        "$$ \\frac{\\partial}{\\partial x_{i}}f\\left(\\boldsymbol{x}_{0}\\right) = \\boldsymbol{g}_{0_{i}}$$\n",
        "As is holds for $i=1,\\dots,d$:\n",
        "$$\\boldsymbol{g}_0=\\begin{bmatrix}\\frac{\\partial}{\\partial x_{1}}f\\left(\\boldsymbol{x}_{0}\\right)\\\\\n",
        "\\frac{\\partial}{\\partial x_{2}}f\\left(\\boldsymbol{x}_{0}\\right)\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial}{\\partial x_{d}}f\\left(\\boldsymbol{x}_{0}\\right)\n",
        "\\end{bmatrix}$$\n",
        "Therefore, by definition: $\\boldsymbol{g}_0 = \\nabla f \\left( \\boldsymbol{x}_0 \\right)$ \n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAeIVs6ZEd0S"
      },
      "source": [
        "**Definition**\n",
        "\n",
        "$f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ is said to be **linear** if:\n",
        "\n",
        "$$ f \\left( \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y} \\right) = \\alpha f \\left( \\boldsymbol{x} \\right) + \\beta f \\left( \\boldsymbol{y} \\right) $$\n",
        "\n",
        "For all $\\alpha, \\beta \\in \\mathbb{R}$ and for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{{d}_{1}}$.\n",
        "\n",
        "\n",
        "\n",
        "Let $f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ be a linear function.\n",
        "\n",
        "### 2.2. Question\n",
        "\n",
        "Prove that:\n",
        "\n",
        "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f \\left( \\boldsymbol{h} \\right) $$\n",
        "\n",
        "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C4AOCrIGEd0S"
      },
      "source": [
        "### 2.2. Solution\n",
        "To prove that $\\nabla f(\\boldsymbol{x})[\\boldsymbol{h}] = f(\\boldsymbol{h})$ for all $\\boldsymbol{x},\\boldsymbol{h}\\in\\mathbb{R}^{d_1}$, we can use the definition of the directional derivative:\n",
        "\n",
        "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = \\lim_{t \\to 0} \\frac{f \\left( \\boldsymbol{x} + t \\boldsymbol{h} \\right) - f \\left( \\boldsymbol{x} \\right)}{t} $$\n",
        "\n",
        "By the linearity of $f$, we can rewrite:\n",
        "\n",
        "\\begin{align*}\n",
        "f \\left( \\boldsymbol{x} + t \\boldsymbol{h} \\right) = f \\left( t \\boldsymbol{h} + \\boldsymbol{x} \\right) \\\n",
        "= t f \\left( \\boldsymbol{h} \\right) + f \\left( \\boldsymbol{x} \\right) \\quad\n",
        "\\end{align*}\n",
        "\n",
        "Substituting this into the definition of the directional derivative, we get:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] &= \\lim_{t \\to 0} \\frac{t f \\left( \\boldsymbol{h} \\right) + f \\left( \\boldsymbol{x} \\right) - f \\left( \\boldsymbol{x} \\right)}{t} \\\n",
        "&= \\lim_{t \\to 0} \\frac{t f \\left( \\boldsymbol{h} \\right)}{t} \\\n",
        "&= f \\left( \\boldsymbol{h} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "Therefore, we have shown that if $f$ is a linear function, then the directional derivative of $f$ at $\\boldsymbol{x}$ in the direction of $\\boldsymbol{h}$ is equal to $f \\left( \\boldsymbol{h} \\right)$ for all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{d_1}$.\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9mUU6fT4Ed0S"
      },
      "source": [
        "### 2.3. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $$\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-NrC-06CEd0S"
      },
      "source": [
        "### 2.3. Solution\n",
        "\n",
        "Using the product rule:\n",
        "\n",
        "$\\nabla\\langle f\\left(\\boldsymbol{x}\\right),g\\left(\\boldsymbol{x}\\right)\\rangle\\left[\\boldsymbol{h}\\right]=\\langle\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right],g\\left(\\boldsymbol{x}\\right)\\rangle+\\langle f\\left(\\boldsymbol{x}\\right),\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]\\rangle$\n",
        "\n",
        "And the linearity property: \n",
        "\n",
        "if $f$ is linear, then: $\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]=f\\left(h\\right)$\n",
        "\n",
        "$f\\left(\\boldsymbol{x}\\right)=\\boldsymbol{x}^{T}\\boldsymbol{A}\\boldsymbol{x}=\\langle\\boldsymbol{x},\\boldsymbol{A}\\boldsymbol{x}\\rangle$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]=\\langle\\boldsymbol{h},\\boldsymbol{A}\\boldsymbol{x}\\rangle+\\langle\\boldsymbol{x},\\boldsymbol{A}\\boldsymbol{h}\\rangle=\\boldsymbol{h^{T}Ax+h^{T}A^{T}x=h^{T}(A+A^{T})x=\\langle\\boldsymbol{(A+A^{T})x},h\\rangle}$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{x}\\right)=\\boldsymbol{(A+A^{T})x}$\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_lFrW87fEd0T"
      },
      "source": [
        "### 2.4. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} $$\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UfBB4k44Ed0T"
      },
      "source": [
        "### 2.4. Solution\n",
        "\n",
        "$\\nabla\\langle f\\left(\\boldsymbol{X}\\right),g\\left(\\boldsymbol{X}\\right)\\rangle\\left[\\boldsymbol{H}\\right]=\\langle\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right],g\\left(\\boldsymbol{X}\\right)\\rangle+\\langle f\\left(\\boldsymbol{X}\\right),\\nabla g\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]\\rangle$\n",
        "\n",
        "$f\\left(\\boldsymbol{X}\\right)=\\boldsymbol{Tr(X}^{T}\\boldsymbol{A}\\boldsymbol{X)}=\\langle\\boldsymbol{X},\\boldsymbol{A}\\boldsymbol{X}\\rangle$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]=\\langle\\boldsymbol{H},\\boldsymbol{A}\\boldsymbol{X}\\rangle+\\langle\\boldsymbol{X},\\boldsymbol{A}\\boldsymbol{H}\\rangle=\\boldsymbol{H^{T}AX+H^{T}A^{T}X=H^{T}(A+A^{T})X=\\langle\\boldsymbol{(A+A^{T})X},H\\rangle}$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)=\\boldsymbol{(A+A^{T})X}$\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jQNjnglPEd0T"
      },
      "source": [
        "### 2.5. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XC-hTmyfEd0T"
      },
      "source": [
        "### 2.5. Solution\n",
        "\n",
        "\n",
        "Let $w\\left(\\boldsymbol{z}\\right)={\\left\\Vert \\boldsymbol{z}\\right\\Vert }_{2}^{2} \\Longrightarrow\\nabla w\\left(\\boldsymbol{z}\\right)=2\\boldsymbol{z}$\n",
        "\n",
        "Let $g\\left(\\boldsymbol{x}\\right)=\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x} \\Longrightarrow\\nabla g\\left(x\\right)\\left[\\boldsymbol{h}\\right]=-\\boldsymbol{Ah}$\n",
        "\n",
        "$f\\left(\\boldsymbol{x}\\right)=(w\\circ g\\left)(\\boldsymbol{x}\\right)$\n",
        "\n",
        "Using the chain rule:\n",
        "\n",
        "$\\nabla ((w\\circ g\\left)(\\boldsymbol{x}\\right))\\left[\\boldsymbol{h}\\right]=\\langle w\\left(\\boldsymbol{g\\left(\\boldsymbol{x}\\right)}\\right),\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]\\rangle$\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]=\\nabla ((w\\circ g\\left)(\\boldsymbol{x}\\right))\\left[\\boldsymbol{h}\\right]=\\langle w\\left(g\\left(\\boldsymbol{x}\\right)\\right),\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]\\rangle$\n",
        "\n",
        "$\\Longrightarrow\\langle2g\\left(\\boldsymbol{x}\\right),-\\boldsymbol{Ah}\\rangle=\\langle2\\left(\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x}\\right),-\\boldsymbol{Ah}\\rangle=\\langle-2A^{T}\\left(\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x}\\right),\\boldsymbol{h}\\rangle$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{x}\\right)=-2A^{T}\\left(\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x}\\right)$\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OLWhViK9Ed0U"
      },
      "source": [
        "### 2.6. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{D \\times d}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$.\n",
        " - ${\\left\\| \\cdot \\right\\|}_{F}^{2}$ is the squared [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), that is, ${\\left\\| \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{X}, \\boldsymbol{X} \\right\\rangle = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{X} \\right\\}$.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CIEd_0W3Ed0U"
      },
      "source": [
        "### 2.6. Solution\n",
        "\n",
        "Let $w\\left(\\boldsymbol{Z}\\right)={\\left\\Vert \\boldsymbol{Z}\\right\\Vert }_{F}^{2} \\Longrightarrow\\nabla w\\left(\\boldsymbol{Z}\\right)=2\\boldsymbol{Z}$\n",
        "\n",
        "Let $g\\left(X\\right)=\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{x} \\Longrightarrow\\nabla g\\left(x\\right)\\left[\\boldsymbol{H}\\right]=-\\boldsymbol{AH}$\n",
        "\n",
        "$f\\left(\\boldsymbol{X}\\right)=w\\circ g\\left(\\boldsymbol{X}\\right)$\n",
        "\n",
        "Using the chain rule:\n",
        "\n",
        "$\\nabla\\langle w\\left(\\boldsymbol{X}\\right)\\circ g\\left(\\boldsymbol{X}\\right)\\rangle\\left[H\\right]=\\langle w\\left(\\boldsymbol{g\\left(\\boldsymbol{X}\\right)}\\right),\\nabla g\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]\\rangle$\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]=\\nabla\\langle w\\circ g\\left(\\boldsymbol{X}\\right)\\rangle\\left[\\boldsymbol{H}\\right]=\\langle w\\left(g\\left(\\boldsymbol{X}\\right)\\right),\\nabla g\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]\\rangle$\n",
        "\n",
        "$\\Longrightarrow\\langle2g\\left(\\boldsymbol{X}\\right),-\\boldsymbol{AH}\\rangle=\\langle2\\left(\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{X}\\right),-\\boldsymbol{AH}\\rangle=\\langle-2A^{T}\\left(\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{X}\\right),\\boldsymbol{H}\\rangle$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)=-2A^{T}\\left(\\boldsymbol{Y}-\\boldsymbol{A}\\boldsymbol{X}\\right)$\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i-jtVRq5Ed0U"
      },
      "source": [
        "### 2.7. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle $$\n",
        "\n",
        "Where $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times D}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QFP0EAc_Ed0U"
      },
      "source": [
        "### 2.7. Solution\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{h}\\right]=\\nabla\\left\\langle \\boldsymbol{X^{T}A},\\boldsymbol{Y^{T}}\\right\\rangle\\left[\\boldsymbol{h}\\right]$ \n",
        "\n",
        "Using the product rule:\n",
        "\n",
        "$\\nabla\\left\\langle \\boldsymbol{X^{T}A},\\boldsymbol{Y^{T}}\\right\\rangle =\\left\\langle \\boldsymbol{H^{T}A},\\boldsymbol{Y^{T}}\\right\\rangle +\\left\\langle \\boldsymbol{X^{T}A},\\boldsymbol{0}\\right\\rangle =\\left\\langle \\boldsymbol{H^{T}A},\\boldsymbol{Y^{T}}\\right\\rangle$ \n",
        "\n",
        "$\\left\\langle \\boldsymbol{H^{T}A},\\boldsymbol{Y^{T}}\\right\\rangle =Tr(\\boldsymbol{A^{T}HY^{T}})=Tr(\\boldsymbol{Y^{T}A^{T}H})=\\left\\langle \\boldsymbol{AY,H}\\right\\rangle$ \n",
        "\n",
        "Thus, \n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{X}\\right) = \\boldsymbol{AY,H}$\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5_DKpzYdEd0U"
      },
      "source": [
        "### 2.8. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
        "\n",
        "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_65bREEd0U"
      },
      "source": [
        "### 2.8. Solution\n",
        "\n",
        "\n",
        "$f\\left(\\boldsymbol{x}\\right)=\\left\\langle \\boldsymbol{a},g\\left(\\boldsymbol{x}\\right)\\right\\rangle$\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]=\\left\\langle \\boldsymbol{a},g\\left(\\boldsymbol{x}\\right)\\right\\rangle\\left[\\boldsymbol{h}\\right]$\n",
        "\n",
        "Using the product rule:\n",
        "\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]=\\langle0,g\\left(\\boldsymbol{x}\\right)\\rangle+\\langle \\boldsymbol{a},\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]\\rangle=\\langle \\boldsymbol{a},\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]\\rangle$ \n",
        "\n",
        "$\\nabla g\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]= \\lim{t\\to0}\\frac{g(\\boldsymbol{x}+t\\boldsymbol{h})-g(\\boldsymbol{x})}{t}=\\lim{t\\to0}\\frac{\\begin{bmatrix}g(x_{1}+th)\\\\\n",
        "g(x_{2}+th)\\\\\n",
        "\\vdots\\\\\n",
        "g(x_{d}+th)\n",
        "\\end{bmatrix}-\\begin{bmatrix}g(x_{1})\\\\\n",
        "g(x_{2})\\\\\n",
        "\\vdots\\\\\n",
        "g(x_{d})\n",
        "\\end{bmatrix}}{t}=\\begin{bmatrix}g'(x_{1})h_{1}\\\\\n",
        "g'(x_{2})h_{2}\\\\\n",
        "\\vdots\\\\\n",
        "g'(x_{d})h_{d}\n",
        "\\end{bmatrix}=g'(\\boldsymbol{x})∘\\boldsymbol{h}$ where $\\circ$ denotes element-wise multiplication.\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{x}\\right)\\left[\\boldsymbol{h}\\right]=\\langle \\boldsymbol{a},g'(\\boldsymbol{x})\\circ\\boldsymbol{h}\\rangle=\\langle g'(\\boldsymbol{x})∘\\boldsymbol{a},\\boldsymbol{h}\\rangle$\n",
        "\n",
        "$\\Longrightarrow\\nabla f \\left( \\boldsymbol{x} \\right)= g’(\\boldsymbol{x})\\circ \\boldsymbol{a}$\n",
        "\n",
        "**Alternative solution:**\n",
        "\n",
        "$f(\\boldsymbol{x}) = a^Tg(\\boldsymbol{x})$. \n",
        "\n",
        "The gradient of this function with respect to $\\boldsymbol{x}$ is:\n",
        "\n",
        "$$\\nabla f(\\boldsymbol{x}) = \\begin{bmatrix} a_1g’(x_1), a_2g’(x_2) \\ \\ldots \\ a_dg’(x_d) \\end{bmatrix} = a \\circ g’(\\boldsymbol{x})$$\n",
        "\n",
        "where $\\circ$ denotes element-wise multiplication and $g’(\\boldsymbol{x})$ is the element-wise derivative of $g(\\boldsymbol{x})$.\n",
        "\n",
        "The directional derivative of $f(\\boldsymbol{x})$ in the direction of a vector $\\boldsymbol{h}$ is given by the dot product of the gradient and the direction vector:\n",
        "\n",
        "$$\\nabla f(\\boldsymbol{x})[\\boldsymbol{h}] = \\nabla f(\\boldsymbol{x}) \\cdot \\boldsymbol{h}$$\n",
        "\n",
        "Thus, the directional derivative in the direction of $\\boldsymbol{h}$ is:\n",
        "\n",
        "$$\\nabla f(\\boldsymbol{x})[\\boldsymbol{h}] = \\langle\\nabla f(\\boldsymbol{x}), \\boldsymbol{h}\\rangle = \\langle a \\circ g’(\\boldsymbol{x}), \\boldsymbol{h}\\rangle$$\n",
        "\n",
        "And the gradient of $\\nabla f \\left( \\boldsymbol{x} \\right)$ is: $a \\circ g’(x)$\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QwoW00QDEd0V"
      },
      "source": [
        "### 2.9. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{A}, \\log \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
        " - The function $\\log \\left( \\cdot \\right)$ is the element wise $\\log$ function: $\\boldsymbol{M} = \\log \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\log \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DAQkcz0kEd0V"
      },
      "source": [
        "### 2.9. Solution\n",
        "\n",
        "Let $g(\\boldsymbol{X}) = \\log\\left(\\boldsymbol{X}\\right)$\n",
        "\n",
        "Based on solution 2.8:\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]=\\langle g'(\\boldsymbol{X})∘\\boldsymbol{A},\\boldsymbol{H}\\rangle$\n",
        "Where if $\\boldsymbol{W}=g'(\\boldsymbol{X})\\implies \\boldsymbol{W} \\left[ i, j \\right] =\\frac{1}{\\boldsymbol{X}\\left[i,j\\right]}$ and $∘$  denotes element-wise multiplication.\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)=g'(\\boldsymbol{X})∘\\boldsymbol{A}$\n",
        "\n",
        "**Alternative solution = \"the wrong way\":**\n",
        "\n",
        "$f\\left(\\boldsymbol{X}\\right)=\\left\\langle \\boldsymbol{A},\\log\\left(\\boldsymbol{X}\\right)\\right\\rangle =Tr(A^{T}\\log\\left(\\boldsymbol{X}\\right))=\\sum_{i=1}^{d}\\sum_{j=1}^{d}a_{ij}log(x_{ij})$\n",
        "\n",
        "$\\Longrightarrow\\frac{\\partial f}{\\partial x_{i_{0}j_{0}}}=\\frac{a_{i_{0}j_{0}}}{x_{i_{0}j_{0}}}$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)=A\\circ Z$\n",
        "\n",
        "Where $Z=\\{Z\\in\\mathbb{R}^{dxd}|z_{ij}=\\frac{1}{x_{ij}}\\}$ and $\\circ$ denotes element-wise multiplication.\n",
        "\n",
        "The directional derivative of $f(\\boldsymbol{x})$ in the direction of a vector $\\boldsymbol{h}$ is given by the dot product of the gradient and the direction vector:\n",
        "\n",
        "$$\\nabla f(\\boldsymbol{x})[\\boldsymbol{h}] = \\nabla f(\\boldsymbol{x}) \\cdot \\boldsymbol{h}$$\n",
        "\n",
        "Thus, the directional derivative in the direction of $\\boldsymbol{H}$ is:\n",
        "\n",
        "$$\\nabla f(\\boldsymbol{X})[\\boldsymbol{H}] = \\langle\\nabla f(\\boldsymbol{X}), \\boldsymbol{H}\\rangle = \\langle A\\circ Z, \\boldsymbol{H}\\rangle$$\n",
        "\n",
        "And the gradient of $\\nabla f \\left( \\boldsymbol{x} \\right)$ is: $A \\circ Z$\n",
        "\n",
        "\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pshLT7LSEd0V"
      },
      "source": [
        "### 2.10. Question\n",
        "\n",
        "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
        " - The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ0uiTDAEd0V"
      },
      "source": [
        "### 2.10. Solution\n",
        "$f\\left(\\boldsymbol{X}\\right)=\\left\\langle \\boldsymbol{a},\\operatorname{Diag}\\left(\\boldsymbol{X}\\right)\\right\\rangle$\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]=\\left\\langle \\boldsymbol{a},\\operatorname{Diag}\\left(\\boldsymbol{X}\\right)\\right\\rangle\\left[\\boldsymbol{H}\\right]$\n",
        "\n",
        "Using the product rule and the linearity of $\\operatorname{Diag}\\left(\\boldsymbol{X}\\right)$:\n",
        "\n",
        "\n",
        "$\\nabla f\\left(\\boldsymbol{X}\\right)\\left[\\boldsymbol{H}\\right]=\\langle0,\\operatorname{Diag}\\left(\\boldsymbol{X}\\right)\\rangle+\\langle \\boldsymbol{a},\\operatorname{Diag}\\left(\\boldsymbol{H}\\right)\\rangle=\\langle a,\\operatorname{Diag}\\left(\\boldsymbol{H}\\right)\\rangle=\\langle \\boldsymbol{a},\\boldsymbol{H}\\circ I_d\\rangle=\\langle \\boldsymbol{a}\\circ I_d,\\boldsymbol{H}\\rangle$ \n",
        "\n",
        "\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)= \\boldsymbol{a}\\circ I_d$\n",
        "\n",
        "**Alternative solution - \"the wrong way\":**\n",
        "\n",
        "$f\\left(\\boldsymbol{X}\\right)=\\left\\langle \\boldsymbol{a},\\operatorname{Diag}\\left(\\boldsymbol{X}\\right)\\right\\rangle =\\boldsymbol{a}^{T}\\operatorname{Diag}\\left(\\boldsymbol{X}\\right)=\\sum_{i=1}^{d}\\boldsymbol{a}_{i}\\boldsymbol{X}_{ii}$\n",
        "\n",
        "$\\Longrightarrow\\frac{\\partial f}{\\partial x_{ij}}={\\begin{cases}\n",
        "a_i: & i=j\\\\\n",
        "0: & i\\neq j\n",
        "\\end{cases}},i,j\\in\\{1,2,\\dots,d\\}$\n",
        "\n",
        "$\\Longrightarrow\\nabla f\\left(\\boldsymbol{X}\\right)=\\boldsymbol{a}\\circ I_d$\n",
        "\n",
        "where $\\circ$ denotes element-wise multiplication and $I_d$ is the identity matrix.\n",
        "\n",
        "The directional derivative of $f(\\boldsymbol{X})$ in the direction of a vector $\\boldsymbol{H}$ is given by the dot product of the gradient and the direction vector. Thus, the directional derivative in the direction of $\\boldsymbol{H}$ is:\n",
        "\n",
        "$$\\nabla f(\\boldsymbol{X})[\\boldsymbol{H}] = \\langle\\nabla f(\\boldsymbol{X}), \\boldsymbol{H}\\rangle = \\langle \\boldsymbol{a} \\circ I_d, \\boldsymbol{H}\\rangle$$\n",
        "\n",
        "And the gradient of $\\nabla f \\left( \\boldsymbol{X} \\right)$ is: $\\boldsymbol{a} \\circ I_d$\n",
        "\n",
        "$\\blacksquare$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9LgeT1jLEd0V"
      },
      "source": [
        "## 3. Constraint Optimization\n",
        "\n",
        "**MinMax**  \n",
        "\n",
        "Let $G \\left( x, y \\right) = \\sin \\left( x + y \\right)$.\n",
        "\n",
        "### 3.1. Question\n",
        "\n",
        "Show that:\n",
        "\n",
        " - $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
        " - $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DXrDKVo4Ed0W"
      },
      "source": [
        "### 3.1. Solution\n",
        "\n",
        "- Let’s first consider $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right)$. For any fixed value of $x$, the maximum value of $G(x,y) = \\sin(x+y)$ with respect to $y$ is 1, since the maximum value of the sine function is 1. Therefore, $\\underset{y}{\\max} G \\left( x, y \\right) = 1$ for any $x$. Taking the minimum with respect to $x$, we have $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
        "\n",
        "- Now, let’s consider $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right)$. For any fixed value of $y$, the minimum value of $G(x,y) = \\sin(x+y)$ with respect to $x$ is -1, since the minimum value of the sine function is -1. Therefore, $\\underset{x}{\\min} G \\left( x, y \\right) = -1$ for any $y$. Taking the maximum with respect to $y$, we have $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1qHL9cSTEd0W"
      },
      "source": [
        "**Rayleigh Quotient**  \n",
        "\n",
        "The _Rayleigh Quotient_ is defined by:\n",
        "\n",
        "$$ f \\left( \\boldsymbol{x} \\right) = \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
        "\n",
        "For some symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$.\n",
        "\n",
        "### 3.2. Question\n",
        "\n",
        "Follow the given steps:\n",
        "\n",
        " - Show that $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} $.\n",
        " - Write the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$.\n",
        " - Show that ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$.  \n",
        "   In other words, the stationary points $\\left( \\boldsymbol{x}, \\lambda \\right)$ are the eigenvectors and eigenvalues of $\\boldsymbol{A}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OrlSd9SCEd0W"
      },
      "source": [
        "### 3.2. Solution\n",
        "\n",
        "$\\textbf{1:}$\n",
        "\n",
        "$$f(z\\cdot x) = \\frac{(zx)^{T}Azx}{(zx)^{T}zx} = \\frac{(zz)x^{T}Ax}{(zz)x^{T}x} = \\frac{x^{T}Ax}{x^{T}x} = f(x) $$\n",
        "<br>\n",
        "\n",
        "Thus, the Rayleigh quotient is not affected by scaling $f(z \\cdot x) = f(x)$. \n",
        "\n",
        "Also:\n",
        "<br>\n",
        " $$f(x) = \\frac{x^{T}Ax}{x^{T}x} = \\frac{x^{T}Ax}{\\langle x,x \\rangle} = \\frac{x^{T}Ax}{||x||_{2}^{2}} $$\n",
        "<br> \n",
        "\n",
        "It's sufficient to find ${\\min}$ in the case of $||x||_{2}^{2} = 1$.\n",
        "\n",
        "hence:\n",
        "$${\\min}_{x}f(x) = \n",
        "  \\left\\{\n",
        "    \\begin{array}{ll}\n",
        "      {\\min}_{x}x^{T}Ax \\\\\n",
        "      s.t ||x||_{2}^{2} = 1\n",
        "    \\end{array}\n",
        "  \\right.\n",
        "  \n",
        "$$\n",
        "<br>$\\textbf{2:}$\n",
        "\n",
        "the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$:\n",
        "\n",
        "$$ \\mathcal{L}(x,\\lambda ) = x^{T}Ax - \\lambda (||x||_{2}^{2} - 1)$$  \n",
        "$\\textbf{3:}$\n",
        "$$ \\mathcal{L}(x,\\lambda ) = x^{T}Ax - \\lambda (||x||_{2}^{2} - 1)$$  \n",
        "$\\frac{d\\mathcal{L}(x,\\lambda )}{dx} = 0$\n",
        "\n",
        "$\\frac{d\\mathcal{L}(x,\\lambda )}{dx} = Ax + x^{T}A - 2\\lambda x = Ax + Ax - 2\\lambda x = 2Ax - 2\\lambda x = 0$\n",
        "\n",
        "$ \\Rightarrow2Ax = 2\\lambda x$\n",
        "\n",
        "$ \\Rightarrow Ax = \\lambda x$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S4M0ALYUEd0X"
      },
      "source": [
        "<img src=\"https://i.imgur.com/qIP5xPv.png\" height=\"700\">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fastai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "8109fb122eb6c397dfc0c7d3e36afa05a5b710419708d04cd847e7b4d4eb8c53"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
