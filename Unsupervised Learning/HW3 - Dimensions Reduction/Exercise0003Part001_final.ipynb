{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGsLMNnj_4t"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 003 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 0.1.000 | 14/05/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CalXRIrBj_42"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0002Part001.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mSZyKe5tj_43"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dQhsCgeqj_44"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
        " - Answer all questions / tasks within the Jupyter Notebook.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Verify the rendering on VS Code.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for questions.\n",
        " - Good Luck!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mS0f2BOLj_46"
      },
      "source": [
        "## 1. Principle Component Analysis (PCA)\n",
        "\n",
        "Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix, that is $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}$ where $\\boldsymbol{\\Lambda}$ is a diagonal matrix.\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Prove the following:\n",
        "\n",
        "$$ \\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {\\lambda}_{i} \\left( A \\right) $$\n",
        "\n",
        "Where $\\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {A}_{ii}$ and ${\\lambda}_{i} \\left( \\boldsymbol{A} \\right) = {\\Lambda}_{ii}$ is the $i$ -th eigen value of $\\boldsymbol{A}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XRk6q8bwj_47"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "$\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}$ where $\\boldsymbol{\\Lambda}$ is a diagonal matrix.\n",
        "\n",
        "$$ \\operatorname{Tr} \\left( A \\right) = \\operatorname{Tr}\\left(\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}\\right) = \\operatorname{Tr}\\left(\\boldsymbol{Q}^{-1}\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\right)= \\operatorname{Tr}\\left(\\boldsymbol{I} \\boldsymbol{\\Lambda} \\right)= \\operatorname{Tr}\\left(\\boldsymbol{\\Lambda} \\right)=\\sum_{i = 1}^{d} {\\lambda}_{i} \\left( A \\right) $$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QT07cnjkj_48"
      },
      "source": [
        "Two square matrices $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ and $\\boldsymbol{B} \\in \\mathbb{R}^{d \\times d}$ are called similar, namely $ \\boldsymbol{A} \\sim \\boldsymbol{B}$ if there exist an invertible matrix $\\boldsymbol{P} \\in \\mathbb{R}^{d \\times d}$ such that:\n",
        "\n",
        "$$ \\boldsymbol{B} = \\boldsymbol{P} \\boldsymbol{A} \\boldsymbol{P}^{-1} $$\n",
        "\n",
        "* <font color='brown'>(**#**)</font> It means all matrices which are diagonalizable are similar to their respective diagonal matrix.\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Prove that if $\\boldsymbol{A}$ is diagonalizable and $\\boldsymbol{A} \\sim \\boldsymbol{B}$ then $\\boldsymbol{A}$ and $\\boldsymbol{B}$ share the same eigen values, namely:\n",
        "\n",
        "$$ \\boldsymbol{A} \\sim \\boldsymbol{B} \\implies {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) \\right\\}}_{i = 1}^{d} = {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{B} \\right) \\right\\}}_{i = 1}^{d} $$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q9b2-IlRj_49"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "If $\\boldsymbol{A}$ is diagonalizable, then there exists an invertible matrix $\\boldsymbol{Q}$ such that  $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}$ and $\\boldsymbol{\\Lambda}$ is diagonal.\n",
        "\n",
        "$$ \\boldsymbol{B} = \\boldsymbol{P} \\boldsymbol{A} \\boldsymbol{P}^{-1} $$\n",
        "\n",
        "Define a matrix $\\boldsymbol{S} = \\boldsymbol{P} \\boldsymbol{Q}$\\\n",
        "Since $\\boldsymbol{P}$ and $\\boldsymbol{Q}$ are both invertible, $\\boldsymbol{S}^{-1} = \\boldsymbol{Q}^{-1} \\boldsymbol{P}^{-1}$ exists.\n",
        "\n",
        "$$ \\boldsymbol{B} = \\boldsymbol{P} \\boldsymbol{A} \\boldsymbol{P}^{-1} =\\boldsymbol{P} \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}\\boldsymbol{P}^{-1}= \\boldsymbol{S} \\boldsymbol{\\Lambda} \\boldsymbol{S}^{-1} $$\n",
        "\n",
        "Therefore, $\\boldsymbol{A}$ and $\\boldsymbol{B}$ share the same diagonal matrix and have the same eigenvalues.\n",
        "$$ {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) \\right\\}}_{i = 1}^{d} = {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{B} \\right) \\right\\}}_{i = 1}^{d} $$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nb2Nh2kj_4-"
      },
      "source": [
        "A symmetric matrix $\\boldsymbol{A} = \\boldsymbol{A}^{T}$ is called a _Symmetric Positive Definite_ (SPD) matrix if either:\n",
        "\n",
        "* All eigen values are positive: $\\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0$.\n",
        "* Any quadratic form is positive: $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$.\n",
        "\n",
        "Namely $\\boldsymbol{A} \\succ 0$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> There is also a _Symmetric Semi Positive Definite Matrix_ (SPSD) which obeys the above with weak inequality.\n",
        "\n",
        "### 1.3. Question\n",
        "\n",
        " 1. Prove the equivalency of the 2 properties, namely:\n",
        "\n",
        "$$ \\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0 \\iff  \\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$$\n",
        "\n",
        " 2. Prove or disprove: $\\boldsymbol{A}^{-1}$ is a _Symmetric Positive Definite_ matrix.\n",
        " 3. Prove or disprove: The SVD of $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$ is also an eigen decomposition."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UXHb56Jvj_4_"
      },
      "source": [
        "### 1.3. Solution\n",
        "\n",
        "1.\\\n",
        "if $\\boldsymbol{A}$ is symmetric, it is diagonalizable and there exists an orthonormal basis of eigenvectors  $\\{\\boldsymbol{u}_1, \\boldsymbol{u}_2.... \\boldsymbol{u}_d\\}$  with corresponding real eigenvalues $\\{\\lambda_1, \\lambda_2.... \\lambda_d\\}$, such that $\\boldsymbol{A}\\boldsymbol{u}_i = \\lambda_i\\boldsymbol{u}_i$.\\\n",
        "$\\forall \\boldsymbol{v} \\in \\mathbb{R}^{d}$ we have a decomposition:\\\n",
        "$\\boldsymbol{v} =v_1\\boldsymbol{u}_1 + v_2\\boldsymbol{u}_2 +... + v_d\\boldsymbol{u}_d = \\sum_{i=1}^d v_i\\boldsymbol{u}_i$\\\n",
        "Therefore, $\\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} = (\\sum_{i=1}^d v_i\\boldsymbol{u}_i) \\boldsymbol{A} (\\sum_{i=1}^d v_i\\boldsymbol{u}_i) = (\\sum_{i=1}^d v_i\\boldsymbol{u}_i)(\\sum_{i=1}^d v_i \\lambda_i\\boldsymbol{u}_i) = \\sum_{i=1}^d \\lambda_i v_i^2$\n",
        "\n",
        "Suppose $\\forall i \\quad \\lambda_i(\\boldsymbol{A})>0$\\\n",
        "This implies that $\\forall \\boldsymbol{v}\\neq 0 \\in \\mathbb{R}^{d} \\quad\\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} = \\sum_{i=1}^d \\lambda_i v_i^2 > 0$\n",
        "\n",
        "Conversly, assuming that $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$\\\n",
        "When we substitute for each of the eigenvectors $\\boldsymbol{u}_i$ to get:\\\n",
        "$\\boldsymbol{u}_i^{T} \\boldsymbol{A} \\boldsymbol{u}_i = \\boldsymbol{u}_i^{T} \\lambda_i \\boldsymbol{u}_i = \\lambda_i \\implies \\lambda_i>0 \\quad \\forall i$\n",
        "\n",
        "\n",
        "\\\n",
        "2.\\\n",
        "If $\\boldsymbol{A}$ is SPD, then $\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1}$ with a diagonal $\\boldsymbol{\\Lambda}$.\\\n",
        "Since $\\forall i \\quad \\lambda_i >0 \\quad \\Lambda^{-1}$ exists and is defined by $(\\Lambda^{-1})_{ii} = (\\Lambda_{ii})^{-1} = \\lambda_i^{-1}$\\\n",
        "If we deine $\\boldsymbol{B} =  \\boldsymbol{Q}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}^{-1}$ we find that:\\\n",
        "$\\boldsymbol{A}\\boldsymbol{B} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1}\\boldsymbol{Q}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}^{-1} = \\boldsymbol{I}$\\\n",
        "$\\boldsymbol{B}\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}^{-1}\\boldsymbol{Q}^{-1}\\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1} = \\boldsymbol{I}$\\\n",
        "$\\implies  \\boldsymbol{B} = \\boldsymbol{A}^{-1}$\n",
        "\n",
        "$\\boldsymbol{A}^{-1}$ is symmetric, since the inverse of a symmetric matrix is a symmetric matrix.\n",
        "\n",
        "In summary, $\\boldsymbol{A}^{-1}$ exists, is symmetric, and is diagonalizable with all positive eigenvalues $\\lambda_i^{-1} > 0$\\\n",
        "$\\implies \\boldsymbol{A}^{-1}$ is SPD\n",
        "\n",
        "\n",
        "3.\\\n",
        "If $\\boldsymbol{A}$ is SPD, then it has an eigen decomposition $\\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{\\Lambda}\\boldsymbol{Q}^{-1}$ with all positive eigenvalues.\n",
        "\n",
        "Consider the SVD of $\\boldsymbol{A}$:\\\n",
        "$\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$\\\n",
        "Vectors in $\\boldsymbol{U}$ are the eigenvectors of $\\boldsymbol{A}\\boldsymbol{A}^T$\\\n",
        "Vectors in $\\boldsymbol{V}$ are the eigenvectors of $\\boldsymbol{A}^T\\boldsymbol{A}$\\\n",
        "For SPD: $\\boldsymbol{A}^T\\boldsymbol{A} = \\boldsymbol{A}\\boldsymbol{A}^T = \\boldsymbol{A}^2 = \\boldsymbol{Q}\\boldsymbol{\\Lambda}^2\\boldsymbol{Q}^{-1}$\\\n",
        "Suppose that all eigenvalues are unique. We know there are no $0$ eigenvalues. Then  $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are either equal to vectors in  $\\boldsymbol{Q}$ or different by a sign.\\\n",
        "The singular values in $\\Sigma$ are the absolute values of the eigenvalues. Since  $\\boldsymbol{A}$ is SPD and all eigenvalues are positive, they are equal:\n",
        "$\\Sigma = \\Lambda$\\\n",
        "Therefore, we must choose the same sign for $\\boldsymbol{U}$ and $\\boldsymbol{V}$ and in fact $\\boldsymbol{U}= \\boldsymbol{V}$\\\n",
        "Both $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are unitary and so  $\\boldsymbol{U}^{-1} = \\boldsymbol{U}^T = \\boldsymbol{V}^{-1} = \\boldsymbol{V}^T$\n",
        "\n",
        "To conclude, if eigenvalues are unique:\\\n",
        "$\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{-1}$\\\n",
        "And so the SVD of a SPD matrix is also an eigen decomposition.\n",
        "\n",
        "If eigenvalues are not unique, we can still choose the same orthogonal basis for $\\boldsymbol{U}$ and $\\boldsymbol{V}$ and get an EVD.\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "81lAKC07j_4_"
      },
      "source": [
        " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
        " * Let $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}$ be the eigen decomposition of $\\boldsymbol{\\Sigma}_{x}$.\n",
        " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
        "\n",
        "\n",
        "### 1.4. Question\n",
        "\n",
        "Prove the following:\n",
        "\n",
        " 1. The mean of the set $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{D} \\right\\}$ is zero, that is, $\\boldsymbol{\\mu}_{z} = \\frac{1}{N} \\sum_{i = 1}^{N} \\boldsymbol{z}_{i} = 0$.\n",
        " 2. The covariance of $\\mathcal{Z}$ is diagonal, that is, $\\boldsymbol{\\Sigma}_{z}$ is diagonal.\n",
        " 3. The pair wise distance is equal, that is, $\\forall i, j: \\; {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RYzonGXDj_5A"
      },
      "source": [
        "### 1.4. Solution\n",
        "\n",
        "1.  $\\boldsymbol{\\mu}_{z} = \\frac{1}{N} \\sum_{i = 1}^{N} \\boldsymbol{z}_{i} =  \\frac{1}{N} \\sum_{i = 1}^{N}\\boldsymbol{U}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right) = \\boldsymbol{U}^{T}\\frac{1}{N} \\sum_{i = 1}^{N} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)= \\boldsymbol{U}^{T}(\\frac{1}{N} \\sum_{i = 1}^{N}  \\boldsymbol{x}_{i}- \\frac{1}{N} \\sum_{i = 1}^{N}\\boldsymbol{\\mu}_{x})  = \\boldsymbol{U}^{T}(\\boldsymbol{\\mu}_{x} -  \\boldsymbol{\\mu}_{x})=0$\n",
        "\n",
        "2. $\\boldsymbol{\\Sigma}_{x}$ is symmetric and therefore in the eigen decomposition $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}$,  $\\boldsymbol{\\Lambda}$ is diagonal and $\\boldsymbol{U}$ is an orthogonal basis.\n",
        "$\\boldsymbol{\\Sigma}_{z} = E({Z}{Z}^T) = E( \\boldsymbol{U}^{T}{X}{X}^T\\boldsymbol{U}) = \\boldsymbol{U}^{T}E({X}{X}^T)\\boldsymbol{U} = \\boldsymbol{U}^T \\boldsymbol{\\Sigma}_{x} \\boldsymbol{U} = \\boldsymbol{U}^T \\boldsymbol{U}\\boldsymbol{\\Lambda} \\boldsymbol{U}\\boldsymbol{U}^{T} = \\boldsymbol{\\Lambda}$\\\n",
        "$\\implies \\boldsymbol{\\Sigma}_{z}$ is diagonal.\n",
        "\n",
        "3.  ${\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}^2_{2} = (\\boldsymbol{z}_{i} - \\boldsymbol{z}_{j})^T(\\boldsymbol{z}_{i} - \\boldsymbol{z}_{j}) =(\\boldsymbol{z}_{i} - \\boldsymbol{z}_{j})^T(\\boldsymbol{z}_{i} - \\boldsymbol{z}_{j}) =(\\boldsymbol{U}^T(\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j}))^T\\boldsymbol{U}^T(\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j}) = (\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j}^T)\\boldsymbol{U}\\boldsymbol{U}^T(\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j}) = (\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j}^T)(\\boldsymbol{x}_{i} - \\boldsymbol{x}_{j})  =  {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}^2_{2}$\\\n",
        "$\\implies {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_2$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QeITzAfgj_5A"
      },
      "source": [
        " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a full rank matrix with $d \\leq D$.\n",
        "\n",
        " * <font color='brown'>(**#**)</font> If a matrix has rank $d$ it means its SVD  has $d$ non zero singular values.\n",
        "\n",
        "\n",
        "### 1.5. Question\n",
        "\n",
        "Show that exists an invertible matrix $\\boldsymbol{M} \\in \\mathbb{R}^{d \\times d}$ such that $\\boldsymbol{O} = \\boldsymbol{U}_{d} \\boldsymbol{M} \\in \\mathbb{R}^{D \\times d}$ is semi orthogonal, that is $\\boldsymbol{O}^{T} \\boldsymbol{O} = \\boldsymbol{I}_{d}$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OKw_ltgRj_5B"
      },
      "source": [
        "### 1.5. Solution\n",
        "\n",
        "$\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ has an SVD decomposition. We can look at the **compact** form of the SVD:\\\n",
        "$\\boldsymbol{U}_{d} = \\tilde{\\boldsymbol{U}}\\boldsymbol{S}\\boldsymbol{V}$\n",
        "\n",
        "In the compact form:\\\n",
        "$\\tilde{\\boldsymbol{U}} \\in \\mathbb{R}^{D \\times d}$,\n",
        "$\\boldsymbol{S} \\in \\mathbb{R}^{d \\times d}$,\n",
        "$\\boldsymbol{V} \\in \\mathbb{R}^{d \\times d}$\n",
        "\n",
        "$\\boldsymbol{S}$ is diagonal, and since $\\boldsymbol{U}_{d}$ is full rank, there are no 0 values on the diagonal and $\\boldsymbol{S}^{-1}$ exists with  $\\boldsymbol{S}^{-1}[i,j]={\\boldsymbol{S}[i,j]}^{-1}$.\\\n",
        "$\\boldsymbol{V}$ is unitary and invertible: $\\boldsymbol{V}^{-1} = \\boldsymbol{V}^T$\\\n",
        "$\\tilde{\\boldsymbol{U}}$ is semi-orthgonal: $\\tilde{\\boldsymbol{U}}^T\\tilde{\\boldsymbol{U}} = \\boldsymbol{I}_{d}$\n",
        "\n",
        "$\\boldsymbol{U}_{d}^T\\boldsymbol{U}_{d} = \\boldsymbol{V}^T\\boldsymbol{S}^T\\tilde{\\boldsymbol{U}}^T \\tilde{\\boldsymbol{U}}\\boldsymbol{S}\\boldsymbol{V} = \\boldsymbol{V}^T\\boldsymbol{S}^T\\boldsymbol{S}\\boldsymbol{V}  = \\boldsymbol{V}^T\\boldsymbol{S}^2\\boldsymbol{V}$\n",
        "\n",
        "If we define: $\\boldsymbol{M} = \\boldsymbol{V}^T\\boldsymbol{S}^{-1}$ we get:\\\n",
        "$\\boldsymbol{O}^{T} \\boldsymbol{O} = (\\boldsymbol{U}_{d} \\boldsymbol{M})^T\n",
        " \\boldsymbol{U}_{d} \\boldsymbol{M}= \\boldsymbol{M}^T\\boldsymbol{U}_{d}^T\n",
        " \\boldsymbol{U}_{d} \\boldsymbol{M}= (\\boldsymbol{S}^{-1})^T\\boldsymbol{V}\\boldsymbol{V}^T\\boldsymbol{S}^2\\boldsymbol{V}\\boldsymbol{V}^T\\boldsymbol{S}^{-1} =  \\boldsymbol{I}_{d}$ \\\n",
        " $\\boldsymbol{O} \\boldsymbol{O}^T =\\boldsymbol{U}_{d} \\boldsymbol{M} (\\boldsymbol{U}_{d} \\boldsymbol{M})^T=\\boldsymbol{U}_{d} \\boldsymbol{M} \\boldsymbol{M}^T\\boldsymbol{U}_{d}^T = \\tilde{\\boldsymbol{U}}\\boldsymbol{S}\\boldsymbol{V} \\boldsymbol{V}^T\\boldsymbol{S}^{-1}(\\boldsymbol{S}^{-1})^T\\boldsymbol{V} \\boldsymbol{V}^T\\boldsymbol{S}^T\\tilde{\\boldsymbol{U}}^T = \\tilde{\\boldsymbol{U}}\\tilde{\\boldsymbol{U}}^T \\neq \\boldsymbol{I}_{D}$\\\n",
        " $\\implies$ with $\\boldsymbol{M} = \\boldsymbol{V}^T\\boldsymbol{S}^{-1}$,\n",
        " $ \\boldsymbol{O}$ is semi-orthgonal\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvqit0D1j_5B"
      },
      "source": [
        " * Consider the data $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
        " * Assume it has zero mean $\\boldsymbol{\\mu}_{x} = \\boldsymbol{X} \\boldsymbol{1}_{N} = \\boldsymbol{0}$.\n",
        " * Its covariance is given by $\\boldsymbol{\\Sigma}_{x} = \\frac{1}{N} \\boldsymbol{X} \\boldsymbol{X}^{T}$.\n",
        "\n",
        "\n",
        "### 1.6. Question (Bonus 2 Points)\n",
        "\n",
        "Prove the following 2 optimization problem are equivalent.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} \\\\\n",
        "\\text{subject to} \\quad & \\begin{aligned}\n",
        "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
        "\\end{aligned}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\arg \\max_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & \\operatorname{Tr} \\left( \\boldsymbol{U}_{d}^{T} \\boldsymbol{\\Sigma}_{x} \\boldsymbol{U}_{d} \\right) \\\\\n",
        "\\text{subject to} \\quad & \\begin{aligned}\n",
        "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
        "\\end{aligned}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Equivalent means the have the same solution, in the case above, same minimizer.\n",
        " * <font color='brown'>(**#**)</font> The above shows the equivalence between the trace form, which decorrelates the data, and the encoder decoder form, which minimized the reconstruction error."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bR6IveD9j_5C"
      },
      "source": [
        "### 1.6. Solution\n",
        "\n",
        "We have shown in a previous exercise that ${\\left\\| A \\right\\|}^2_{F} = \\operatorname{Tr}(A^{T}A)$\n",
        "\n",
        "\n",
        "${\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} = \\operatorname{Tr}((\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})^T(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})) = \\operatorname{Tr}(\\boldsymbol{X}^T - \\boldsymbol{X}^T\\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} )(\\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})) = \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{X}) + \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T}\\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X}) - \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{U}_{d}  \\boldsymbol{U}_{d}^{T} \\boldsymbol{X}) - \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{U}_{d}  \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})\n",
        "=\\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{X}) + \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X}) - 2\\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X}) = \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{X}) - \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X})$\n",
        "\n",
        "With permutation inside the trace operator we get:\n",
        "${\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} = \\operatorname{Tr}(\\boldsymbol{X}^T\\boldsymbol{X}) - \\operatorname{Tr}(\\boldsymbol{U}_{d}^{T} \\boldsymbol{X}\\boldsymbol{X}^T\\boldsymbol{U}_{d})  = \\operatorname{Tr}(\\boldsymbol{\\Sigma}_{x}) - \\operatorname{Tr}(\\boldsymbol{U}_{d}^{T} \\boldsymbol{\\Sigma}_{x}^T\\boldsymbol{U}_{d} )$\n",
        "\n",
        "We then see that optimizing for minimal reconstruction error is the same as maximizing the variance in the reduced dimension. Since the total varince $\\operatorname{Tr}(\\boldsymbol{\\Sigma}_{x})$ is constant and does not depend on $\\boldsymbol{U}_{d}$, both optimizations are equivalent:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} \\\\\n",
        "\\text{subject to} \\quad & \\begin{aligned}\n",
        "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
        "\\end{aligned}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\arg \\max_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & \\operatorname{Tr} \\left( \\boldsymbol{U}_{d}^{T} \\boldsymbol{\\Sigma}_{x} \\boldsymbol{U}_{d} \\right) \\\\\n",
        "\\text{subject to} \\quad & \\begin{aligned}\n",
        "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
        "\\end{aligned}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Aobwzxuij_5C"
      },
      "source": [
        " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
        " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a semi orthogonal matrix, that is, $\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} = \\boldsymbol{I}$.\n",
        " * Where $\\boldsymbol{U}_{d}$ be the $d$ eigen vectors corressponding to the $d$ largest eigen values of $\\boldsymbol{\\Sigma}_{x}$.\n",
        " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}_{d}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
        " * Let $\\hat{\\boldsymbol{x}}_{i} = \\boldsymbol{U}_{d} \\boldsymbol{z}_{i} + \\boldsymbol{\\mu}_{x}$.\n",
        " * Let $\\boldsymbol{\\epsilon}_{i} = \\hat{\\boldsymbol{x}}_{i} - \\boldsymbol{x}_{i}$.\n",
        "\n",
        "\n",
        "### 1.7. Question\n",
        "\n",
        "Prove the following:\n",
        "\n",
        " 1. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{x} \\right) = \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{z} \\right) + \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right)$.\n",
        " 2. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\sum_{i = d + 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$.\n",
        "\n",
        "Where\n",
        "\n",
        " * $\\boldsymbol{\\Sigma}_{z} \\in \\mathbb{R}^{d \\times d}$ is the covariance of $\\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$.\n",
        " * $\\boldsymbol{\\Sigma}_{\\epsilon} \\in \\mathbb{R}^{D \\times D}$ is the covariance of $\\left\\{ \\boldsymbol{\\epsilon}_{i} \\right\\}_{i = 1}^{N}$.\n",
        "\n",
        "</br>\n",
        "\n",
        " * <font color='brown'>(**#**)</font> The idea is to prove the total variance / energy is kept. Part of it in the low dimensionality data and the other is in the error (Lost information).\n",
        " * <font color='brown'>(**#**)</font> To make calculations easier, think how $\\boldsymbol{\\mu}_{x}$ effects the variance.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmi1gioGj_5D"
      },
      "source": [
        "### 1.7. Solution\n",
        "\n",
        "1.\\\n",
        "$\\hat{\\boldsymbol{x}}_{i} = \\boldsymbol{U}_{d} \\boldsymbol{z}_{i} + \\boldsymbol{\\mu}_{x} = \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right) + \\boldsymbol{\\mu}_{x} =  \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\boldsymbol{x}_{i} + (1-\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T})\\boldsymbol{\\mu}_{x} =  \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\boldsymbol{x}_{i} + \\boldsymbol{c}$\\\n",
        "$\\underline{\\hat{\\boldsymbol{X}}} = \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}+ \\boldsymbol{c}$\n",
        "\n",
        "$Var(\\underline{\\boldsymbol{\\mathcal{E}}}) = E[(\\underline{\\hat{\\boldsymbol{X}}} - \\underline{\\boldsymbol{X}})^T(\\underline{\\hat{\\boldsymbol{X}}} - \\underline{\\boldsymbol{X}})]  = E[(\\underline{\\boldsymbol{X}} - \\underline{\\hat{\\boldsymbol{X}}})^T(\\underline{\\boldsymbol{X}} - \\underline{\\hat{\\boldsymbol{X}}})] = E[(\\underline{\\boldsymbol{X}} - \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}- \\boldsymbol{c})^T(\\underline{\\boldsymbol{X}} - \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}- \\boldsymbol{c})]$\\\n",
        "Since an added constant does not change the variance of a random variable, we have:\\\n",
        "$Var(\\underline{\\boldsymbol{\\mathcal{E}}}) = E[(\\underline{\\boldsymbol{X}} - \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}})^T(\\underline{\\boldsymbol{X}} - \\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}})] = $\n",
        "\n",
        "\n",
        "$E[\\underline{\\boldsymbol{X}}^{T}\\underline{\\boldsymbol{X}}]+E[ \\underline{\\boldsymbol{X}}^{T}\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}] - E[ \\underline{\\boldsymbol{X}}^{T}\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}]- E[ \\underline{\\boldsymbol{X}}^{T}\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}]= E[\\underline{\\boldsymbol{X}}^{T}\\underline{\\boldsymbol{X}}]+E[ \\underline{\\boldsymbol{X}}^{T}\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}] - 2E[ \\underline{\\boldsymbol{X}}^{T}\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}]=\n",
        "E[\\underline{\\boldsymbol{X}}^{T}\\underline{\\boldsymbol{X}}]- E[ \\underline{\\boldsymbol{X}}^{T}\\boldsymbol{U}_{d}\\boldsymbol{U}_{d}^{T}\\underline{\\boldsymbol{X}}]= E[\\underline{\\boldsymbol{X}}^{T}\\underline{\\boldsymbol{X}}] - E[\\underline{\\boldsymbol{Z}}^{T}\\underline{\\boldsymbol{Z}}] = Var(\\underline{\\boldsymbol{X}}) - Var(\\underline{\\boldsymbol{Z}})$\n",
        "\n",
        "(again for $\\underline{\\boldsymbol{Z}}$ - the average does not change the variance)\n",
        "\n",
        "Since $Var(\\underline{\\boldsymbol{X}}) = \\operatorname{Tr}(\\boldsymbol{\\Sigma}_{x}) $ we get:\\\n",
        " $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{x} \\right) = \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{z} \\right) + \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right)$\n",
        "\n",
        "\n",
        "2.\\\n",
        "Suppose $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U}\\boldsymbol{\\Lambda}\\boldsymbol{U}^{T} \\quad (\\iff \\boldsymbol{\\Sigma}_{x}\\boldsymbol{U} = \\boldsymbol{U}\\boldsymbol{\\Lambda})$\\\n",
        "$\\operatorname{Tr}(\\boldsymbol{\\Sigma}_{x}) = \\operatorname{Tr}(\\boldsymbol{U}\\boldsymbol{\\Lambda}\\boldsymbol{U}^{T})= \\operatorname{Tr}(\\boldsymbol{U}^{T}\\boldsymbol{U}\\boldsymbol{\\Lambda}) =\\operatorname{Tr}(\\boldsymbol{\\Lambda}) = \\sum_{i = 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$\\\n",
        "Similarly:$(\\boldsymbol{\\Sigma}_{x}\\boldsymbol{U}_d = \\boldsymbol{U}_d\\boldsymbol{\\Lambda}_d)$\\\n",
        "$\\operatorname{Tr}(\\boldsymbol{\\Sigma}_{z}) = \\operatorname{Tr}(\\boldsymbol{U}_d\\boldsymbol{\\Lambda}_d\\boldsymbol{U}_d^{T})= \\operatorname{Tr}(\\boldsymbol{U}_d^{T}\\boldsymbol{U}_d\\boldsymbol{\\Lambda}_d) =\\operatorname{Tr}(\\boldsymbol{\\Lambda}_d) = \\sum_{i = 1}^{d} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$\n",
        "\n",
        "\n",
        "$\\implies \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{x} \\right) - \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{z} \\right) = \\sum_{i = 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)- \\sum_{i = 1}^{d} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right) =  \\sum_{i = d+1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5YO3ktlj_5D"
      },
      "source": [
        " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a square matrix.\n",
        " * Let $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\in \\mathbb{R}^{d}$ be eigen vectors with the same eigen value $\\lambda$ such that $\\boldsymbol{A} \\boldsymbol{u}_{1} = \\lambda \\boldsymbol{u}_{1}$ and $\\boldsymbol{A} \\boldsymbol{u}_{2} = \\lambda \\boldsymbol{u}_{2}$.\n",
        " * The vectors $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2}$ are perpendicular, that is, $\\left \\langle \\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\right \\rangle = 0$.\n",
        " * Let $\\boldsymbol{v} = \\alpha \\boldsymbol{u}_{1}$ where $\\alpha \\neq 0$.\n",
        "\n",
        "\n",
        "### 1.10. Question\n",
        "\n",
        " 1. Show $\\boldsymbol{v}$ is an eigen vector of $\\boldsymbol{A}$ and find its corresponding eigen value.\n",
        " 2. Prove or disprove that $\\boldsymbol{u} = \\boldsymbol{u}_{1} + \\boldsymbol{u}_{2}$ is an eigen vector of $A$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6r8iiRr0j_5D"
      },
      "source": [
        "### 1.10. Solution\n",
        "\n",
        "1.\\\n",
        "$\\boldsymbol{v}$ is an eigen vector of $\\boldsymbol{A}$ with corresponding eigen value $\\lambda$:\\\n",
        "$\\boldsymbol{A} \\boldsymbol{v} = \\boldsymbol{A} \\alpha \\boldsymbol{u}=  \\alpha\\boldsymbol{A} \\boldsymbol{u} = \\alpha\\lambda\\boldsymbol{u}= \\lambda\\alpha\\boldsymbol{u}=  \\lambda\\boldsymbol{v}$\n",
        "\n",
        "2.\\\n",
        "$\\boldsymbol{u} = \\boldsymbol{u}_{1} + \\boldsymbol{u}_{2}$ is an eigen vector of $A$:\\\n",
        "$\\boldsymbol{A} \\boldsymbol{u} = \\boldsymbol{A} (\\boldsymbol{u}_1 + \\boldsymbol{u}_2) = \\boldsymbol{A}\\boldsymbol{u}_1 + \\boldsymbol{A}\\boldsymbol{u}_2= \\lambda\\boldsymbol{u}_1 + \\lambda\\boldsymbol{u}_1= \\lambda (\\boldsymbol{u}_1 + \\boldsymbol{u}_2) =  \\lambda\\boldsymbol{u}$\\\n",
        "So $\\boldsymbol{u}$ is an eigenvector of $\\boldsymbol{A}$ (as are all linear combinations of eigenvectors with the same eigenvalue).\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7CaeNgWVj_5E"
      },
      "source": [
        " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix.\n",
        " * Let $\\left\\{ \\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} \\right) \\right\\}_{i = 1}^{d}$ be the set of eigen pairs, that is, $\\boldsymbol{A} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
        " * Let $\\boldsymbol{B} = \\boldsymbol{R} \\boldsymbol{A} \\boldsymbol{R}^{T}$ where $\\boldsymbol{R} \\in \\mathbb{R}^{d \\times d}$ is an orthogonal matrix, namely, $\\boldsymbol{R}^{T} \\boldsymbol{R} = \\boldsymbol{R} \\boldsymbol{R}^{T} = \\boldsymbol{I}$.\n",
        "\n",
        "\n",
        "### 1.11. Question\n",
        "\n",
        "Find the set $\\left\\{ \\left( \\boldsymbol{v}_{i}, {\\alpha}_{i} \\right) \\right\\}_{i = 1}^{d}$ of eigen pairs of $\\boldsymbol{B}$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ymQf798wj_5E"
      },
      "source": [
        "### 1.11. Solution\n",
        "\n",
        "\n",
        "Define: $\\boldsymbol{v}_{i} = \\boldsymbol{R}\\boldsymbol{u}_{i}$\\\n",
        "With this definition:\\\n",
        "$\\boldsymbol{B}\\boldsymbol{v}_{i} = \\boldsymbol{R} \\boldsymbol{A} \\boldsymbol{R}^{T}\\boldsymbol{R}\\boldsymbol{u}_{i} = \\boldsymbol{R} \\boldsymbol{A} \\boldsymbol{u}_{i} =\\boldsymbol{R} \\lambda_{i} \\boldsymbol{u}_{i} =\\lambda_{i}\\boldsymbol{R}\\boldsymbol{u}_{i} =\\lambda_{i}\\boldsymbol{v}_{i}$\\\n",
        "$\\implies$ the set $\\left\\{ \\left( \\boldsymbol{R}\\boldsymbol{u}_{i}, {\\lambda}_{i} \\right) \\right\\}_{i = 1}^{d}$ are eigen pairs of $\\boldsymbol{B}$\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QMOSmeP_j_5F"
      },
      "source": [
        " * Consider the data $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{10} \\right\\}_{i = 1}^{N}$ and its matrix form $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times N}$.\n",
        " * Let $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{10 \\times 10}$ be the covariance matrix where all of its eigen values are unique.\n",
        " * Let $\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{3} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{3}$\n",
        " * Let $\\left\\{ \\boldsymbol{w}_{i} \\in \\mathbb{R}^{2} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{2}$\n",
        "\n",
        "\n",
        "### 1.12. Question\n",
        "\n",
        "Prove or disprove:\n",
        "\n",
        "$$ \\boldsymbol{w}_{i} = \\begin{bmatrix}\n",
        "{\\alpha}_{1} & 0 & 0 \\\\\n",
        "0 & {\\alpha}_{2} & 0\n",
        "\\end{bmatrix} \\boldsymbol{z}_{i}, \\; {\\alpha}_{1}, {\\alpha}_{2} \\in \\left\\{ -1, 1 \\right\\} $$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TfhjpTaBj_5F"
      },
      "source": [
        "### 1.12. Solution\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U}\\boldsymbol{\\Lambda}\\boldsymbol{U}^{T}$\\\n",
        "$\\boldsymbol{z}_{i} = \\boldsymbol{U}_3^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$\\\n",
        "$\\boldsymbol{w}_{i} = \\tilde{\\boldsymbol{U}}_2^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$\\\n",
        "$\\boldsymbol{U}_3 = \\left[\n",
        "  \\begin{array}{cccc}\n",
        "    | & | &  | \\\\\n",
        "    u_{1}  & u_{2} & u_{3}    \\\\\n",
        "    | & | &   |\n",
        "  \\end{array}\n",
        "\\right]$\\\n",
        "$\\tilde{\\boldsymbol{U}}_2 = \\left[\n",
        "  \\begin{array}{cccc}\n",
        "    | & |  \\\\\n",
        "    \\tilde{u}_{1}  & \\tilde{u}_{2}    \\\\\n",
        "    | & |\n",
        "  \\end{array}\n",
        "\\right]$\\\n",
        "$u_{1},u_{2}, u_{3}$ are all eigenvectors of $\\boldsymbol{\\Sigma}_{x}$ and since all eigenvalues are unique, the normalized eigenvectors are also unique up to a sign.\\\n",
        "Since vectors are ordered according to decreasing eigenvalue, this means that:\\\n",
        "$\\boldsymbol{u}_{1} = \\pm \\tilde{\\boldsymbol{u}}_1$\\\n",
        "$\\boldsymbol{u}_{2} = \\pm \\tilde{\\boldsymbol{u}}_2$\n",
        "\n",
        "\n",
        "To construct a transition matrix $\\boldsymbol{A}$ between the bases of $\\boldsymbol{U}_3$ and $\\tilde{\\boldsymbol{U}}_2$ we can write:\\\n",
        "$\\boldsymbol{A}\\boldsymbol{u}_{1} = \\tilde{\\boldsymbol{u}}_1 = \\pm\\boldsymbol{u}_{1}$\\\n",
        "$\\boldsymbol{A}\\boldsymbol{u}_{2} = \\tilde{\\boldsymbol{u}}_2 = \\pm\\boldsymbol{u}_{2}$\\\n",
        "$\\boldsymbol{A}\\boldsymbol{u}_{3} = 0$\n",
        "\n",
        "$\\implies \\boldsymbol{w}_{i} = \\tilde{\\boldsymbol{U}}_2^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)=\\boldsymbol{A}\\boldsymbol{U}_3^{T}\\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)=\\boldsymbol{A}\\boldsymbol{z}_{i}\\quad$, with:\\\n",
        "$$ \\boldsymbol{A} = \\begin{bmatrix}\n",
        "{\\alpha}_{1} & 0 & 0 \\\\\n",
        "0 & {\\alpha}_{2} & 0\n",
        "\\end{bmatrix}, \\; {\\alpha}_{1}, {\\alpha}_{2} \\in \\left\\{ -1, 1 \\right\\} $$\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BfJ2mYqKj_5F"
      },
      "source": [
        "## 2. Kernel Principle Component Analysis (K-PCA)\n",
        "\n",
        "Let $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T} \\in \\mathbb{R}^{N \\times N}$ be the centering matrix.\n",
        "\n",
        "### 2.1. Question\n",
        "\n",
        "Prove that $\\boldsymbol{J}$ is an idempotent matrix, that is, $\\boldsymbol{J}^{k} = \\boldsymbol{J}$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> This implies that if we center the data multiple times it will have no effect beyond doing it once."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GjQRjs3pj_5G"
      },
      "source": [
        "### 2.1. Solution\n",
        "\n",
        "$\\boldsymbol{J}^2 = (\\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T})(\\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}) = \\boldsymbol{I} - \\frac{2}{N}\\boldsymbol{1} \\boldsymbol{1}^{T} + \\frac{1}{N^2}\\boldsymbol{1} \\boldsymbol{1}^{T}\\boldsymbol{1} \\boldsymbol{1}^{T}  = \\boldsymbol{I} - \\frac{2}{N}\\boldsymbol{1} \\boldsymbol{1}^{T} + \\frac{1}{N^2}\\boldsymbol{1} N \\boldsymbol{1}^{T} = \\boldsymbol{I} - \\frac{2}{N}\\boldsymbol{1} \\boldsymbol{1}^{T} + \\frac{1}{N}\\boldsymbol{1}\\boldsymbol{1}^{T}  = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T} =  \\boldsymbol{J}$\n",
        "From here it is easy to see by induction that $\\boldsymbol{J}^k = \\boldsymbol{J}$:\\\n",
        "$\\boldsymbol{J}^k =\\boldsymbol{J}^{k-1}\\boldsymbol{J} = \\boldsymbol{J}\\boldsymbol{J}= \\boldsymbol{J}^2 = \\boldsymbol{J}$\\\n",
        "Where $k=2$ is also the induction base.\n",
        "\n",
        "$\\implies \\boldsymbol{J}$ is an idempotent matrix\n",
        "\n",
        "-------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hXSfLn3Oj_5G"
      },
      "source": [
        " * Let $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
        " * The covariance matrix $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{X} \\boldsymbol{X}^{T} \\in \\mathbb{R}^{D \\times D}$.\n",
        " * The matrix $\\boldsymbol{K}_{x} = \\boldsymbol{X}^{T} \\boldsymbol{X} \\in \\mathbb{R}^{N \\times N}$.\n",
        " * Let $\\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} > 0 \\right)$ be an eigen pair of $\\boldsymbol{\\Sigma}_{x}$, such that, $\\boldsymbol{\\Sigma}_{x} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
        "\n",
        "\n",
        "### 2.2. Question\n",
        "\n",
        " 1. Show that ${\\lambda}_{i}$ is an eigen value of $\\boldsymbol{K}_{x}$.\n",
        " 2. Find the corresponding eigen vector $\\boldsymbol{v}_{i}$ such that $\\boldsymbol{K}_{x} \\boldsymbol{v}_{i} = {\\lambda}_{i} \\boldsymbol{v}_{i}$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pk0fehH_j_5G"
      },
      "source": [
        "### 2.2. Solution\n",
        "1.+2.\\\n",
        "If $\\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} > 0 \\right)$ is an eigen pair of $\\boldsymbol{\\Sigma}_{x}$, such that, $\\boldsymbol{\\Sigma}_{x} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$, then:\\\n",
        "$\\boldsymbol{X} \\boldsymbol{X}^{T}\\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
        "\n",
        "We can define: $\\boldsymbol{v}_{i} = \\boldsymbol{X}^{T}\\boldsymbol{u}_{i} \\in \\mathbb{R}^{d} $\\\n",
        "Then $\\boldsymbol{K}_{x}\\boldsymbol{v}_{i} = \\boldsymbol{X}^T \\boldsymbol{X}\\boldsymbol{X}^{T}\\boldsymbol{u}_{i} = \\boldsymbol{X}^T{\\lambda}_{i} \\boldsymbol{u}_{i} = {\\lambda}_{i}\\boldsymbol{X}^T \\boldsymbol{u}_{i} = {\\lambda}_{i}\\boldsymbol{v}_{i}$\n",
        "\n",
        "Therefore, $\\boldsymbol{v}_{i} = \\boldsymbol{X}^{T}\\boldsymbol{u}_{i} $ is an eigenvector of $\\boldsymbol{K}_{x}$ with eigenvalue ${\\lambda}_{i}$.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K6SweYa1j_5G"
      },
      "source": [
        " **Kernel Functions**\n",
        "\n",
        " * Let $k: \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\to \\mathbb{R}$.\n",
        " * Consider $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{N}$.\n",
        "\n",
        "### 2.3. Question\n",
        "\n",
        "Show that if $k \\left( \\cdot, \\cdot \\right)$ can be written as inner product:\n",
        "\n",
        "$$ k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle $$\n",
        "\n",
        "for some $\\phi : \\mathbb{R}^{d} \\to \\mathbb{R}^{M} $ then the matrix defined by $\\boldsymbol{K} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) $ is _Symmetric Positive Semi Definite_ (SPSD), that is, $\\boldsymbol{K} \\succeq 0$.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dT-HCU-7j_5H"
      },
      "source": [
        "### 2.3. Solution\n",
        "\n",
        "$\\boldsymbol{K}$ is a symmetric matrix, since the inner product is symmetric:\\\n",
        "$\\boldsymbol{K} \\left[ i, j \\right] = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle= \\left \\langle \\phi \\left( \\boldsymbol{x}_{j} \\right), \\phi \\left( \\boldsymbol{x}_{i} \\right) \\right \\rangle= \\boldsymbol{K} \\left[ j, i \\right]$\n",
        "\n",
        "Next, consider the set $\\Phi = \\left\\{ \\phi(\\boldsymbol{x}_{i}) \\right\\}_{i = 1}^{N}$.\n",
        "\n",
        "\n",
        "We can write: $\\boldsymbol{K} \\left[ i, j \\right] = \\phi \\left( \\boldsymbol{x}_{i} \\right)^T \\phi \\left( \\boldsymbol{x}_{j} \\right)$\\\n",
        "In matrix form: $\\boldsymbol{K} = \\Phi^T\\Phi$\n",
        "\n",
        "\n",
        "For every $\\boldsymbol{v} \\in \\mathbb{R}^{d} \\quad \\boldsymbol{v}^T\\boldsymbol{K}\\boldsymbol{v} = \\boldsymbol{v}^T\\Phi^T\\Phi\\boldsymbol{v} = \\left \\langle \\Phi\\boldsymbol{v}, \\Phi\\boldsymbol{v} \\right \\rangle \\geq 0 $\n",
        "$\\implies \\boldsymbol{K} \\succeq 0$ (SPSD)\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tqsRgrKlj_5H"
      },
      "source": [
        " * Let $\\boldsymbol{A} \\succ 0 $ be an SPD matrix.\n",
        " * Let $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\boldsymbol{x}_{i}^{T} \\boldsymbol{A} \\boldsymbol{x}_{j} $.\n",
        "\n",
        "### 2.4. Question\n",
        "\n",
        "Prove or disprove that $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ is a kernel function."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "19IgFHTIj_5H"
      },
      "source": [
        "### 2.4. Solution\n",
        "Since $\\boldsymbol{A} \\succ 0 $ is an SPD matrix, $\\boldsymbol{A}$ is congruent with a diagonal matrix with all positive eigenvalues:\\\n",
        " $\\boldsymbol{A} = \\boldsymbol{P}^T\\boldsymbol{\\Lambda} \\boldsymbol{P}$\n",
        "\n",
        "Since all eigenvalues are positive, we can easily take the square root of the matrix $\\boldsymbol{\\Lambda}$: $\\quad\\boldsymbol{\\Lambda} = \\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{\\Lambda}^{\\frac 1 2} $, with $\\quad {\\boldsymbol{\\Lambda}^{\\frac 1 2}}_{ii} = \\sqrt{\\boldsymbol{\\Lambda}_{ii}}$\n",
        "\n",
        "\n",
        "\n",
        " $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\boldsymbol{x}_{i}^{T} \\boldsymbol{A} \\boldsymbol{x}_{j}  = \\boldsymbol{x}_{i}^{T}  \\boldsymbol{P}^T\\boldsymbol{\\Lambda} \\boldsymbol{P} \\boldsymbol{x}_{j} = \\boldsymbol{x}_{i}^{T}  \\boldsymbol{P}^T\\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{\\Lambda}^{\\frac 1 2}  \\boldsymbol{P} \\boldsymbol{x}_{j} = \\boldsymbol{x}_{i}^{T}  \\boldsymbol{P}^T{\\boldsymbol{\\Lambda}^{\\frac 1 2}}^T\\boldsymbol{\\Lambda}^{\\frac 1 2}  \\boldsymbol{P} \\boldsymbol{x}_{j}\\quad $ ($\\boldsymbol{\\Lambda}^{\\frac 1 2}$ is symmetric)\n",
        "\n",
        "\n",
        "Now we can define the mapping: $\\phi: \\boldsymbol{x}_{i} \\rightarrow  \\boldsymbol{\\Lambda}^{\\frac 1 2}  \\boldsymbol{P} \\boldsymbol{x}_{i}$\n",
        "\n",
        "With this:$\\quad k\\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) =  \\phi \\left( \\boldsymbol{x}_{i}\\right)^T \\phi \\left( \\boldsymbol{x}_{j}\\right) = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle$\n",
        "\n",
        "$k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ can now be written as an explicit inner product and is therefore a kernel function.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gxHX6x2Gj_5H"
      },
      "source": [
        "\n",
        "Let $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left( 1 + \\boldsymbol{x}^{T} \\boldsymbol{y} \\right)}^{2}$.\n",
        "\n",
        "### 2.5. Question\n",
        "\n",
        "Prove or disprove that $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right)$ is a kernel function."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_w2uI-Iqj_5I"
      },
      "source": [
        "### 2.5. Solution\n",
        "We can prove this by finding an explicit inner product expression for the kernal function.\\\n",
        "$k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left( 1 + \\boldsymbol{x}^{T} \\boldsymbol{y} \\right)}^{2} = (1+x_1y_1+x_2y_2+\\dots ++x_dy_d)^2 =\n",
        "1 + \\sum_{i=1}^dx_i^2y_i^2 + 2\\sum_{i=1}^dx_iy_i + 2\\sum_{i\\ne j}x_iy_ix_jy_j$\n",
        "\n",
        "With this, we can define the mapping: $\\phi: \\boldsymbol{x} \\rightarrow  (1, x_1^2, x_2^2, \\dots, x_d^2, \\sqrt{2}x_1, \\sqrt{2}x_2, \\dots, \\sqrt{2}x_d, \\sqrt{2}x_1x_2, \\sqrt{2}x_1x_3, \\dots, \\sqrt{2}x_1x_d, \\sqrt{2}x_2x_3, \\sqrt{2}x_2x_4, \\dots, \\sqrt{2}x_2x_d, \\dots,\\sqrt{2}x_ix_j(i \\ne j), \\dots \\sqrt{2}x_{d-1}x_d)$\\\n",
        "With this mapping:\\\n",
        "$k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle$\\\n",
        "and so $k$ is a kernel function.\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nnvaEQVKj_5I"
      },
      "source": [
        "\n",
        " * Let $\\boldsymbol{K}_{x} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ be an **SPD** kernel matrix.\n",
        " * Let $\\tilde{\\boldsymbol{K}}_{x} = \\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J}$ be a centered kernel matrix where $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$.\n",
        "\n",
        "### 2.7. Question\n",
        "\n",
        "Prove or disprove: $\\tilde{\\boldsymbol{K}}_{x}$ is an SPD matrix."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "74NQMPVlj_5I"
      },
      "source": [
        "### 2.7. Solution\n",
        "\n",
        "\n",
        "$\\tilde{\\boldsymbol{K}}_{x}$ is not an SPD matrix.\n",
        "The problem is that $\\boldsymbol{J}$ is not a full rank matrix. It has an eigenvalue of $0$ along the vector of $1$s.\\\n",
        "Therefore for $\\boldsymbol{1}$ (non trivial vector): $\\quad \\boldsymbol{1}^T\\tilde{\\boldsymbol{K}}_{x}\\boldsymbol{1} = \\boldsymbol{1}^T\\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J}\\boldsymbol{1} = 0$\n",
        "\n",
        "It is however an SPSD.\\\n",
        "We can write for $\\boldsymbol{K}_{x}$\\\n",
        "$\\boldsymbol{K}_{x} = \\boldsymbol{P}^T\\boldsymbol{\\Lambda}\\boldsymbol{P}$ for a diagnal $\\boldsymbol{\\Lambda}$ with all positive eigenvalues. We can then write:\\\n",
        "$\\tilde{\\boldsymbol{K}}_{x} = \\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J} = \\boldsymbol{J} \\boldsymbol{P}^T\\boldsymbol{\\Lambda}^{\\frac 1 2} \\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{P} \\boldsymbol{J}$\\\n",
        "$\\implies \\boldsymbol{v}^T\\boldsymbol{J}^T \\boldsymbol{P}^T\\boldsymbol{\\Lambda}^{\\frac 1 2} \\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{P} \\boldsymbol{J}\\boldsymbol{v} = \\left \\langle \\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{P} \\boldsymbol{J}\\boldsymbol{v}, \\boldsymbol{\\Lambda}^{\\frac 1 2}\\boldsymbol{P} \\boldsymbol{J}\\boldsymbol{v}\\right \\rangle$\\\n",
        "Since $\\boldsymbol{P}$ is full rank then for all $\\boldsymbol{v}$ in the image space of $\\boldsymbol{J}$ this expression is positive. Therefore $\\tilde{\\boldsymbol{K}}_{x}$ is SPSD.\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9yVslw0vj_5I"
      },
      "source": [
        "**Out of Sample Extension**\n",
        "\n",
        " * Consider the training set $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
        " * Let $\\boldsymbol{K}_{x}$ be the kernel matrix obtained by applying the kernel on the training set.\n",
        " * Let $\\boldsymbol{J} \\boldsymbol{K} \\boldsymbol{J} = \\boldsymbol{V} \\boldsymbol{\\Sigma}^{2} \\boldsymbol{V}^{T}$ be the eigen decomposition of the centered kernel matrix.\n",
        " * Let $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$ be the low dimensional representation obtained by applying K-PCA, that is, $\\boldsymbol{Z} = \\boldsymbol{\\Sigma}_{d} \\boldsymbol{V}_{d}^{T}$.\n",
        "\n",
        "</br>\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Kernel matrix is also called _Gram Matrix_.\n",
        "\n",
        "### 2.8. Question (Bonus 2 Points)\n",
        "\n",
        "Let $\\boldsymbol{X}^{\\star} \\in \\mathbb{R}^{D \\times M}$ be a set of a new unseen data points. Write an expression, in a matrix form, for $\\boldsymbol{Z}^{\\star} \\in \\mathbb{R}^{d \\times M}$, the K-PCA out of sample extension applied to $\\boldsymbol{X}^{\\star}$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iqQL9F7_j_5J"
      },
      "source": [
        "### 2.8. Solution\n",
        "\n",
        "If we had the mapping $\\phi: \\boldsymbol{x}_i \\rightarrow\\boldsymbol{\\Phi}_i \\in \\mathbb{R}^{\\bar{D}}$ for the kernel matrix, then we could write:\\\n",
        "$\\boldsymbol{J} \\boldsymbol{K} \\boldsymbol{J} = \\boldsymbol{J} \\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} \\boldsymbol{J} = \\tilde{\\boldsymbol{\\Phi}}^T \\tilde{\\boldsymbol{\\Phi}}$\n",
        "\n",
        "For out-of-sample encoding we would like to find:\\\n",
        "$\\boldsymbol{Z}^{\\star} = \\boldsymbol{\\Sigma}_d^{-1}\\boldsymbol{V}^T\\tilde{\\boldsymbol{\\Phi}}^T \\tilde{\\boldsymbol{\\phi}^{\\star}}$  with $\\tilde{\\boldsymbol{\\phi}^{\\star}} \\in \\mathbb{R}^{\\bar{D}\\times M}$\n",
        "\n",
        "$\\tilde{\\boldsymbol{\\Phi}}^T \\tilde{\\boldsymbol{\\phi}^{\\star}} = \\boldsymbol{J} \\boldsymbol{\\Phi}^T\\boldsymbol{\\phi}^{\\star}\\boldsymbol{J}$\n",
        "\n",
        "$\\boldsymbol{\\phi}^{\\star}$ needs to be centered by the in-sample data:\\\n",
        "$\\boldsymbol{\\phi}^{\\star}\\boldsymbol{J} = \\boldsymbol{\\phi}^{\\star} - \\frac 1 N \\boldsymbol{\\Phi}\\boldsymbol{1}_N \\boldsymbol{1}_M^T$\\\n",
        "$\\implies \\boldsymbol{J} \\boldsymbol{\\Phi}^T\\boldsymbol{\\phi}^{\\star}\\boldsymbol{J} = \\boldsymbol{J} \\boldsymbol{\\Phi}^T (\\boldsymbol{\\phi}^{\\star} - \\frac 1 N \\boldsymbol{\\Phi}\\boldsymbol{1}_N \\boldsymbol{1}_M^T) = \\boldsymbol{J} (\\boldsymbol{\\Phi}^T \\boldsymbol{\\phi}^{\\star} - \\frac 1 N \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi}\\boldsymbol{1}_N \\boldsymbol{1}_M^T) = \\boldsymbol{J} (\\boldsymbol{k}^{\\star} - \\frac 1 N \\boldsymbol{K}\\boldsymbol{1}_N \\boldsymbol{1}_M^T)$\\\n",
        "Where, \\\n",
        "$\\boldsymbol{k}^{\\star} \\in \\mathbb{R}^{N\\times M} $ is a matrix such that $\\boldsymbol{k}^{\\star}[i,j] = k(\\boldsymbol{x}_i, \\boldsymbol{x}^{\\star}_j), \\quad \\boldsymbol{x}^{\\star}_j \\in \\{ \\boldsymbol{X}^{\\star}\\}_{i = 1}^M$\n",
        "\n",
        "Now we can write the expression using only the kernel function (no mapping):\\\n",
        "$\\boldsymbol{Z}^{\\star} = \\boldsymbol{\\Sigma}_d^{-1}\\boldsymbol{V}^T\\boldsymbol{J} (\\boldsymbol{k}^{\\star} - \\frac 1 N \\boldsymbol{K}\\boldsymbol{1}_N \\boldsymbol{1}_M^T)$\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
