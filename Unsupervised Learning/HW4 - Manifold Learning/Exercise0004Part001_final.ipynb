{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxXbapjtbPHo"
      },
      "source": [
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "# UnSupervised Learning Methods\n",
        "\n",
        "## Exercise 004 - Part I\n",
        "\n",
        "> Notebook by:\n",
        "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
        "\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        |Content / Changes                                                   |\n",
        "|---------|------------|-------------|--------------------------------------------------------------------|\n",
        "| 0.1.000 | 11/06/2023 | Royi Avital | First version                                                      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNXTgqvibPHp"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0004Part001.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYsJ9Rq8bPHq"
      },
      "source": [
        "## Notations\n",
        "\n",
        "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
        "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
        "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
        "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNstSBXXbPHq"
      },
      "source": [
        "## Guidelines\n",
        "\n",
        " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
        " - Answer all questions / tasks within the Jupyter Notebook.\n",
        " - Use MarkDown + MathJaX + Code to answer.\n",
        " - Verify the rendering on VS Code.\n",
        " - Submission in groups (Single submission per group).\n",
        " - You may and _should_ use the forums for questions.\n",
        " - Good Luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4v0V9EZbPHw"
      },
      "source": [
        "## 1. Classic Multi Dimensional Scaling (MDS)\n",
        "\n",
        " * Given a function $ \\phi \\left( \\cdot \\right) : \\mathbb{R}^{D} \\to \\mathbb{R}^{M} $.\n",
        " * Consider the following inner product: ${\\left \\langle \\boldsymbol{x}, \\boldsymbol{y} \\right \\rangle}_{\\phi} = \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle$.\n",
        " * Yields the induced norm: $ {\\left\\| \\boldsymbol{x} \\right\\|}_{\\phi} = \\sqrt{ \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{x} \\right) \\right \\rangle } $.\n",
        " * Yields the induced metric: ${d}_{\\phi} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\boldsymbol{x} - \\boldsymbol{y} \\right\\|}_{\\phi}$.\n",
        "\n",
        "### 1.1. Question\n",
        "\n",
        "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D}_{\\phi} \\left[ i, j \\right] = {d}_{\\phi}^{2} \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$.\n",
        "\n",
        "Show that $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$ where:\n",
        "\n",
        " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
        " * $\\boldsymbol{K}_{\\phi} = \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi}$ where: $\\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\phi \\left( \\boldsymbol{x}_{1} \\right) & \\phi \\left( \\boldsymbol{x}_{2} \\right) & \\dots & \\phi \\left( \\boldsymbol{x}_{N} \\right) \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N} = \\boldsymbol{\\Phi} = \\begin{bmatrix} \\mid & \\mid &  & \\mid \\\\ \\boldsymbol{\\phi}_{1} & \\boldsymbol{\\phi}_{2} & \\dots & \\boldsymbol{\\phi}_{N} \\\\ \\mid & \\mid & & \\mid \\end{bmatrix} \\in \\mathbb{R}^{M \\times N}$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> Hints:\n",
        "* Show that the transformation $\\phi \\left( \\cdot \\right)$ must be linear: $\\phi \\left( \\alpha \\boldsymbol{x}, \\beta \\boldsymbol{y} \\right) = \\alpha \\phi \\left( \\boldsymbol{x} \\right) + \\beta \\phi \\left( \\boldsymbol{y} \\right)$.  \n",
        "You may use $\\left \\langle \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle$ as a starting point.\n",
        "* Show that ${d}_{\\phi}^{2} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2} = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle + {\\left\\| \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2}$\n",
        "* Use the lecture notes to conclude $- \\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzhiLrI-bPHx"
      },
      "source": [
        "### 1.1. Solution\n",
        "\n",
        "Consider the inner product: ${\\left \\langle \\alpha\\boldsymbol{x}+\\beta\\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle}_{\\phi}$\\\n",
        "for some $\\boldsymbol{x}, \\boldsymbol{y}, \\boldsymbol{z} \\in \\mathbb{R}^{D}, \\alpha, \\beta \\in \\mathbb{R}$\n",
        "\n",
        "Since $\\left \\langle \\cdot \\right \\rangle_{\\phi}$ is an inner product we must have by linearity:\\\n",
        " ${\\left \\langle \\alpha\\boldsymbol{x}+\\beta\\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle}_{\\phi} =   \\alpha{\\left \\langle\\boldsymbol{x}, \\boldsymbol{z} \\right \\rangle}_{\\phi}+\\beta{\\left \\langle\\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle}_{\\phi} \\stackrel{\\text{by definition}}{=} \\alpha{\\left \\langle \\phi(\\boldsymbol{x}), \\phi(\\boldsymbol{z}) \\right \\rangle}+\\beta{\\left \\langle\\phi(\\boldsymbol{y}), \\phi(\\boldsymbol{z}) \\right \\rangle} ={\\left \\langle \\alpha\\phi(\\boldsymbol{x}), \\phi(\\boldsymbol{z}) \\right \\rangle}+{\\left \\langle\\beta\\phi(\\boldsymbol{y}), \\phi(\\boldsymbol{z}) \\right \\rangle} \\stackrel{\\text{by linearity of Euclidean inner product}}{=} \\left \\langle \\alpha\\phi(\\boldsymbol{x})+\\beta\\phi(\\boldsymbol{y}), \\phi(\\boldsymbol{z}) \\right \\rangle$\n",
        "\n",
        " We then have: ${\\left \\langle \\alpha\\boldsymbol{x}+\\beta\\boldsymbol{y}, \\boldsymbol{z} \\right \\rangle}_{\\phi} = \\left \\langle \\phi(\\alpha\\boldsymbol{x}+\\beta\\boldsymbol{y}), \\phi(\\boldsymbol{z}) \\right \\rangle = \\left \\langle \\alpha\\phi(\\boldsymbol{x})+\\beta\\phi(\\boldsymbol{y}), \\phi(\\boldsymbol{z}) \\right \\rangle$\n",
        "\n",
        "\n",
        "Since this is true for all $\\phi(\\boldsymbol{z})$ we must conclude that $\\phi(\\alpha\\boldsymbol{x}+\\beta\\boldsymbol{y}) = \\alpha\\phi(\\boldsymbol{x})+\\beta\\phi(\\boldsymbol{y}) \\quad \\forall \\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{D}, \\alpha, \\beta \\in \\mathbb{R}$\\\n",
        "$\\implies \\phi(\\cdot)$ is linear.\n",
        "\n",
        "\n",
        "${d}_{\\phi}^{2} \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left\\| \\boldsymbol{x} -\\boldsymbol{y} \\right\\|}_{\\phi}^{2} = \\left \\langle \\phi(\\boldsymbol{x-y}), \\phi(\\boldsymbol{x-y}) \\right \\rangle \\stackrel{\\phi(\\cdot)\\text{ is linear}}{=}  \\left \\langle \\phi(\\boldsymbol{x}) - \\phi(\\boldsymbol{y}), \\phi(\\boldsymbol{x}) - \\phi(\\boldsymbol{y}) \\right \\rangle = \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{x} \\right) \\right \\rangle -2\\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle + \\left \\langle \\phi \\left( \\boldsymbol{y} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle =$\\\n",
        "${\\left\\| \\phi \\left( \\boldsymbol{x} \\right) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi \\left( \\boldsymbol{x} \\right), \\phi \\left( \\boldsymbol{y} \\right) \\right \\rangle + {\\left\\| \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2} = {\\left\\| \\phi \\left( \\boldsymbol{x} \\right) - \\phi \\left( \\boldsymbol{y} \\right) \\right\\|}_{2}^{2}$\n",
        "\n",
        "\n",
        " $\\boldsymbol{D}_{\\phi} \\left[ i, j \\right] = {d}_{\\phi}^{2} \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = {\\left\\| \\phi (\\boldsymbol{x_i}) \\right\\|}_{2}^{2} - 2 \\left \\langle \\phi (\\boldsymbol{x_i}), \\phi( \\boldsymbol{x_j}) \\right \\rangle + {\\left\\| \\phi( \\boldsymbol{x_j}) \\right\\|}_{2}^{2}$\n",
        "\n",
        "\n",
        "In matrix form, this can be written as:\n",
        "\n",
        "$\\boldsymbol{D}_{\\phi} = \\boldsymbol{P}_{\\phi} -2\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}+ \\boldsymbol{P}^T_{\\phi}$\\\n",
        "where,\\\n",
        "$\\boldsymbol{P}_{\\phi} = \\begin{bmatrix} \\left\\| \\phi (\\boldsymbol{x_1}) \\right\\|_{2}^{2} \\\\ \\left\\| \\phi (\\boldsymbol{x_2}) \\right\\|_{2}^{2} \\\\ \\vdots \\\\ \\left\\| \\phi (\\boldsymbol{x_2}) \\right\\|_{2}^{N} \\end{bmatrix} \\boldsymbol{1}_N^T$\n",
        "\n",
        "Since $\\boldsymbol{P}_{\\phi}$ has constant rows, subtracting the row mean vector yields the $\\boldsymbol{0}$ matrix. Therefore:\n",
        "\n",
        "$\\boldsymbol{P}_{\\phi} \\boldsymbol{J}  =\\boldsymbol{P}_{\\phi} (\\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}) = \\boldsymbol{0}$\n",
        "\n",
        "Also: $(\\boldsymbol{P}_{\\phi} \\boldsymbol{J})^T =  \\boldsymbol{J}^T\\boldsymbol{P}^T_{\\phi} = \\boldsymbol{J}\\boldsymbol{P}^T_{\\phi} = \\boldsymbol{0}$\\\n",
        "$\\implies \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = \\boldsymbol{J} (\\boldsymbol{P}_{\\phi} -2\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}+ \\boldsymbol{P}^T_{\\phi}) \\boldsymbol{J} = -2\\boldsymbol{J}\\Phi^T\\Phi\\boldsymbol{J}$\\\n",
        "$\\implies -\\frac{1}{2} \\boldsymbol{J} \\boldsymbol{D}_{\\phi} \\boldsymbol{J} = J \\boldsymbol{K}_{\\phi} \\boldsymbol{J}$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CpN-uPybPHy"
      },
      "source": [
        "Consider the data (Training set) $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ and let $\\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2}$.\n",
        "\n",
        "### 1.2. Question\n",
        "\n",
        "Show that $\\boldsymbol{v}^{T} \\boldsymbol{D} \\boldsymbol{v} \\leq 0$ for any $\\boldsymbol{v}$ such that $\\left \\langle \\boldsymbol{v}, \\boldsymbol{1} \\right \\rangle = 0$.  \n",
        "What does it imply on _distance matrices_?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SiaK8pHbPHy"
      },
      "source": [
        "### 1.2. Solution\n",
        "\n",
        "$\\left \\langle \\boldsymbol{v}, \\boldsymbol{1} \\right \\rangle = 0 \\implies \\boldsymbol{v}^T \\boldsymbol{1} = \\boldsymbol{1}^T\\boldsymbol{v} = 0$\n",
        "\n",
        "If we write:\n",
        "$\\boldsymbol{D} = \\boldsymbol{P} -2\\boldsymbol{X}^T\\boldsymbol{X}+ \\boldsymbol{P}^T$\\\n",
        "$\\boldsymbol{v}^{T} \\boldsymbol{D} \\boldsymbol{v} = \\boldsymbol{v}^{T} (\\boldsymbol{P} -2\\boldsymbol{X}^T\\boldsymbol{X}+ \\boldsymbol{P}^T) \\boldsymbol{v}  = \\boldsymbol{v}^{T} \\boldsymbol{P}\\boldsymbol{v} -2\\boldsymbol{v}^{T} \\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{v} + \\boldsymbol{v}^{T} \\boldsymbol{P}^T \\boldsymbol{v}$\n",
        "\n",
        "Since $\\boldsymbol{P}$ is of the form $\\boldsymbol{p}\\boldsymbol{1}^T$,\\\n",
        "$\\boldsymbol{v}^{T} \\boldsymbol{P}\\boldsymbol{v}= \\boldsymbol{v}^{T}\\boldsymbol{p}\\boldsymbol{1}^T\\boldsymbol{v} = \\boldsymbol{v}^{T}\\boldsymbol{p}\\stackrel{=0}{(\\boldsymbol{1}^T\\boldsymbol{v})} = 0$, and\\\n",
        "$\\boldsymbol{v}^{T} \\boldsymbol{P}^T\\boldsymbol{v}= \\boldsymbol{v}^{T}\\boldsymbol{1}\\boldsymbol{p}^T\\boldsymbol{v} = \\stackrel{=0}{(\\boldsymbol{v}^{T}\\boldsymbol{1})}\\boldsymbol{p}^T\\boldsymbol{v} = 0$\n",
        "\n",
        "\n",
        "$\\implies \\boldsymbol{D} = -2\\boldsymbol{v}^{T} \\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{v} = -2 \\left \\langle \\boldsymbol{X}\\boldsymbol{v}, \\boldsymbol{X}\\boldsymbol{v} \\right \\rangle \\le 0$, since the inner prodcut is non negative by definition.\n",
        "\n",
        "\n",
        "We understand from this that distance matrices are negative semi-definite on the subspace perpendicular to $\\boldsymbol{1}$. This implies that $\\boldsymbol{D}$ has $N-1$ negative eigenvalues. Since the trace of $\\boldsymbol{D}$ is 0, and the trace must be preserves in diagonalization, the remaining eigenvalue must be positive (only one positive value).\\\n",
        "When applying centering by $\\boldsymbol{J} \\boldsymbol{D} \\boldsymbol{J}$, the subspace spanned by $\\boldsymbol{1}$ is sent to $0$ (recall $\\boldsymbol{J}\\boldsymbol{1} = \\boldsymbol{0}$).\\\n",
        "Finally by adding a minus sign we obtain the kernel $\\boldsymbol{K} = -\\frac 1 2 \\boldsymbol{J} \\boldsymbol{D} \\boldsymbol{J}$ as a full SPSD.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8TZ2HP7bPHy"
      },
      "source": [
        "## 3. Metric Multi Dimensional Scaling (MDS)\n",
        "\n",
        "The metric MDS objective is given by:\n",
        "\n",
        "$$ \\arg \\min_{\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}} {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} $$\n",
        "\n",
        "Where:\n",
        "\n",
        " * $\\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] = d \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ - The given distance matrix.\n",
        " * $\\boldsymbol{D}_{z} = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}$.\n",
        "\n",
        "Consider the surrogate function:\n",
        "\n",
        "$$ g \\left( \\boldsymbol{Z}, \\tilde{\\boldsymbol{Z}} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right) - 4 \\left \\langle \\boldsymbol{Z}^{T} \\tilde{\\boldsymbol{Z}}, \\boldsymbol{B} \\right \\rangle $$\n",
        "\n",
        "Where:\n",
        "\n",
        " * $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$ - The centering matrix.\n",
        " * $\\tilde{\\boldsymbol{D}}_{\\tilde{z}} \\left[ i, j \\right] = {\\left\\| \\tilde{\\boldsymbol{z}}_{i} - \\tilde{\\boldsymbol{z}}_{j} \\right\\|}_{2}$.\n",
        " * $\\boldsymbol{C} \\left[ i, j \\right] = \\begin{cases} 0 & \\text{ if } i = j \\\\ - \\frac{ \\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] }{ \\tilde{\\boldsymbol{D}}_{z} \\left[ i, j \\right] } & \\text{ if } i \\neq j \\end{cases}$.\n",
        " * $\\boldsymbol{B} = \\boldsymbol{C} - \\operatorname{Diag} \\left( \\boldsymbol{C} \\boldsymbol{1} \\right)$.\n",
        "\n",
        "### 3.1. Question\n",
        "\n",
        "Prove that $ \\boldsymbol{B} \\boldsymbol{J} = \\boldsymbol{B} $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFnrkowkbPHz"
      },
      "source": [
        "### 3.1. Solution\n",
        "With $\\beta_{ij}  = \\frac{ \\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] }{ \\tilde{\\boldsymbol{D}}_{z} \\left[ i, j \\right] }$\n",
        "$$\\boldsymbol{C} = \\begin{pmatrix}0 & -\\beta_{12} & -\\beta_{13} & \\cdots & -\\beta_{1N}\\\\\n",
        "-\\beta_{21} & 0 & -\\beta_{23} & \\cdots & -\\beta_{2N}\\\\\n",
        "\\vdots & &  & \\ddots & \\\\\n",
        "-\\beta_{N1} & -\\beta_{N2} & -\\beta_{N3} & \\cdots & 0\\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "$$\\boldsymbol{C}\\boldsymbol{1}= \\begin{bmatrix} \\sum_{i\\ne 1}{-\\beta_{1i}}\\\\\n",
        "\\sum_{i\\ne 2}{-\\beta_{2i}}\\\\\n",
        "\\vdots\\\\\n",
        "\\sum_{i\\ne N}{-\\beta_{Ni}}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$\\boldsymbol{B} = \\boldsymbol{C} - \\operatorname{Diag} \\left( \\boldsymbol{C} \\boldsymbol{1} \\right) = \\begin{pmatrix}\\sum_{i\\ne 1}{\\beta_{1i}} & -\\beta_{12} & -\\beta_{13} & \\cdots & -\\beta_{1N}\\\\\n",
        "-\\beta_{21} & \\sum_{i\\ne 2}{\\beta_{2i}}& -\\beta_{23} & \\cdots & -\\beta_{2N}\\\\\n",
        "\\vdots & &  & \\ddots & \\\\\n",
        "-\\beta_{N1} & -\\beta_{N2} & -\\beta_{N3} & \\cdots & \\sum_{i\\ne N}{\\beta_{Ni}}\\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "By the construction, it is easy to see that the row sums of $\\boldsymbol{B}$ are 0 and therefore $\\boldsymbol{B}\\boldsymbol{J} = \\boldsymbol{B}$\n",
        "\n",
        "In another way:\n",
        "$\\boldsymbol{B}\\boldsymbol{J} = \\boldsymbol{C}\\boldsymbol{J} - \\operatorname{Diag} (\\boldsymbol{C} \\boldsymbol{1})\\boldsymbol{J} = \\boldsymbol{C}\\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{C}\\boldsymbol{1} \\boldsymbol{1}^{T} - \\operatorname{Diag} (\\boldsymbol{C} \\boldsymbol{1})\\boldsymbol{I} + \\frac{1}{N} \\operatorname{Diag} (\\boldsymbol{C} \\boldsymbol{1})\\boldsymbol{1} \\boldsymbol{1}^{T}$\\\n",
        "We note that $\\operatorname{Diag} (\\boldsymbol{C} \\boldsymbol{1})\\boldsymbol{1} = \\boldsymbol{C} \\boldsymbol{1}$\\\n",
        "This is True since the only non-zero elements of $\\operatorname{Diag} (\\boldsymbol{C} \\boldsymbol{1})$ are on the diagonal and they contain the row sums, which are the elements of $\\boldsymbol{C} \\boldsymbol{1}$. Therefore, \\\n",
        "$\\boldsymbol{B}\\boldsymbol{J} = \\boldsymbol{C} - \\frac{1}{N} \\boldsymbol{C}\\boldsymbol{1} \\boldsymbol{1}^{T} - \\operatorname{Diag} (\\boldsymbol{C} \\boldsymbol{1}) + \\frac{1}{N} \\boldsymbol{C}\\boldsymbol{1} \\boldsymbol{1}^{T} = \\boldsymbol{C} - \\operatorname{Diag} (\\boldsymbol{C} \\boldsymbol{1}) = \\boldsymbol{B}$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwR6P7RZbPHz"
      },
      "source": [
        "### 3.2. Question\n",
        "\n",
        "Show that $g \\left( \\boldsymbol{Z}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
        "\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Hints (See _lecture notes_):\n",
        "  * ${\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} - 2 \\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle + {\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
        "  * ${\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right)$.\n",
        "  * $\\boldsymbol{D}^{\\circ 2}_{z} \\left[ i, j \\right] = \\boldsymbol{p} \\boldsymbol{1}^{T} - 2 \\boldsymbol{Z}^{T} \\boldsymbol{Z} + 1 \\boldsymbol{p}^{T}, \\; \\boldsymbol{p} = \\begin{bmatrix} {\\left\\| \\boldsymbol{z}_{1} \\right\\|}_{2}^{2} \\\\ {\\left\\| \\boldsymbol{z}_{2} \\right\\|}_{2}^{2} \\\\ \\vdots {\\left\\| \\boldsymbol{z}_{N} \\right\\|}_{2}^{2} \\end{bmatrix}$.\n",
        "  * For $\\tilde{\\boldsymbol{Z}} = \\boldsymbol{Z}$ we have $\\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle = - \\left \\langle \\boldsymbol{C}, \\boldsymbol{D}^{\\circ 2}_{z} \\right \\rangle$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLXBPBQjbPHz"
      },
      "source": [
        "### 3.2. Solution\n",
        "We defined:\n",
        "$ g \\left( \\boldsymbol{Z}, \\tilde{\\boldsymbol{Z}} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right) - 4 \\left \\langle \\boldsymbol{Z}^{T} \\tilde{\\boldsymbol{Z}}, \\boldsymbol{B} \\right \\rangle$\\\n",
        "so:\n",
        "$ g \\left( \\boldsymbol{Z}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right) - 4 \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\boldsymbol{B} \\right \\rangle$\n",
        "\n",
        "We have: ${\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} - 2 \\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle + {\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$.\n",
        "\n",
        "We also showed: ${\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} = 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right)$\n",
        "\n",
        "From all of the above, what's left to show is that:\\\n",
        "$\\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle = 2\\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\boldsymbol{B} \\right \\rangle$\n",
        "\n",
        "We can follow the lecture notes, where we used the Cauchy-Schwarz inequality, but now we can use equality:\\\n",
        "$\\boldsymbol{D}_{z}[i,j] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = \\frac{\\left \\langle \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j}, \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j}\\right \\rangle}{{\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}}= \\frac{\\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\boldsymbol{A}_{ij}\\right \\rangle}{{\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}}= \\frac{\\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\boldsymbol{A}_{ij}\\right \\rangle}{\\boldsymbol{D}_{z}[i,j]} = \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\frac{\\boldsymbol{A}_{ij}}{\\boldsymbol{D}_{z}[i,j]}\\right \\rangle$\\\n",
        "with:\\\n",
        "$\\boldsymbol{A}_{ij}[i,i] = \\boldsymbol{A}_{ij}[j,j] = 1$\\\n",
        "$\\boldsymbol{A}_{ij}[i,j] = \\boldsymbol{A}_{ij}[j,i] = -1$\\\n",
        "and $0$ elsewhere.\\\n",
        "$\\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle = \\sum_{i,j = 1}^N{\\boldsymbol{\\Delta}_x[i,j]\\boldsymbol{D}_z[i,j]} \\stackrel{\\text{distance diagonal is 0}}{=} \\sum_{i\\neq j}^N{\\boldsymbol{\\Delta}_x[i,j]\\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\frac{\\boldsymbol{A}_{ij}}{\\boldsymbol{D}_{z}[i,j]}\\right \\rangle} = \\sum_{i\\neq j}^N{\\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\frac{\\boldsymbol{\\Delta}_x[i,j]}{\\boldsymbol{D}_{z}[i,j]}\\boldsymbol{A}_{ij}\\right \\rangle}= \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z},\\sum_{i\\neq j}^N{ \\frac{\\boldsymbol{\\Delta}_x[i,j]}{\\boldsymbol{D}_{z}[i,j]}\\boldsymbol{A}_{ij}}\\right \\rangle = \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z},2\\sum_{i<j}^N{ \\frac{\\boldsymbol{\\Delta}_x[i,j]}{\\boldsymbol{D}_{z}[i,j]}\\boldsymbol{A}_{ij}}\\right \\rangle = \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z},2\\sum_{i<j}^N{\\boldsymbol{\\beta}_{ij}\\boldsymbol{A}_{ij}}\\right \\rangle = \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z},2\\boldsymbol{B}\\right \\rangle = 2\\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z},\\boldsymbol{B}\\right \\rangle$\n",
        "\n",
        "With $\\beta_{ij}  = \\frac{ \\boldsymbol{\\Delta}_{x} \\left[ i, j \\right] }{ \\boldsymbol{D}_{z} \\left[ i, j \\right] }$\n",
        "\n",
        "To conclude:\\\n",
        "$ g \\left( \\boldsymbol{Z}, \\boldsymbol{Z} \\right) = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + 2 N \\operatorname{Tr} \\left( \\boldsymbol{Z} \\boldsymbol{J} \\boldsymbol{Z}^{T} \\right) - 4 \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\boldsymbol{B} \\right \\rangle = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + {\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} - 4 \\left \\langle \\boldsymbol{Z}^{T} \\boldsymbol{Z}, \\boldsymbol{B} \\right \\rangle = {\\left\\| \\boldsymbol{\\Delta}_{x} \\right\\|}_{F}^{2} + {\\left\\| \\boldsymbol{D}_{z} \\right\\|}_{F}^{2} - 2\\left \\langle \\boldsymbol{\\Delta}_{x}, \\boldsymbol{D}_{z} \\right \\rangle = {\\left\\| \\boldsymbol{\\Delta}_{x} - \\boldsymbol{D}_{z} \\right\\|}_{F}^{2}$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Q8pAiQbPHz"
      },
      "source": [
        "## 4. IsoMap\n",
        "\n",
        "Let $G = \\left( V, E, W \\right) $ be a simple, undirected and weighted graph with no negative weights / edges.  \n",
        "Let $\\boldsymbol{D} \\in \\mathbb{R}^{N \\times N}$ be the shortest path distance matrix where $ \\left| V \\right| = N$.\n",
        "\n",
        "\n",
        "### 4.1. Question\n",
        "\n",
        "Prove or disprove: There is an embedding ${\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{d} \\right\\}}_{i = 1}^{N}$ for some $d \\in \\mathbb{N}$ such that:\n",
        "\n",
        "$$ \\forall i, j \\; \\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "4QaL5tC34VUD",
        "outputId": "8fb39258-1b8d-4ffc-bbaa-9e01b4f1ceb3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython import display\n",
        "display.Image(\"https://i.stack.imgur.com/wPpwH.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr-sk9kobPH0"
      },
      "source": [
        "### 4.1. Solution\n",
        "\n",
        "Disprove by counter example.\\\n",
        "Take for example an octahedron (a double pyramid) as in the image above.\n",
        "Suppose all the edges of the octahedron are of length 1 and that we only connect vertices within distance up to 1 from one another. This means that vertices A and C as in the image are not directly connected. Their distance from one another on the graph is 2.\\\n",
        "Now consider the triangle A-E-C.\\\n",
        "If we try to follow the constraints of the questions, then:\n",
        "$$ \\forall i, j \\; \\boldsymbol{D} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} $$\n",
        "\n",
        "$ \\boldsymbol{D} \\left[ A, E \\right] = {\\left\\| \\boldsymbol{z}_{A} - \\boldsymbol{z}_{E} \\right\\|}_{2} = 1$\\\n",
        "$ \\boldsymbol{D} \\left[ E, C \\right] = {\\left\\| \\boldsymbol{z}_{E} - \\boldsymbol{z}_{C} \\right\\|}_{2} = 1$\\\n",
        "$ \\boldsymbol{D} \\left[ A, C \\right] = {\\left\\| \\boldsymbol{z}_{A} - \\boldsymbol{z}_{C} \\right\\|}_{2} = 2$\\\n",
        "In whatever embedding space we choose, if these are the euclidean distances then A, E and C must lie on a straight line.\n",
        "\n",
        "The exact same argument applies to the triangle A-D-C.\n",
        "Therefore, in whatever embedding A, D and C must also lie on a straight line.\n",
        "But this implies that the distance between E and D is 0:\\\n",
        "${\\left\\| \\boldsymbol{z}_{E} - \\boldsymbol{z}_{D} \\right\\|}_{2} = 0$\\\n",
        "But we require:\\\n",
        "$ \\boldsymbol{D} \\left[ E, D \\right] = {\\left\\| \\boldsymbol{z}_{E} - \\boldsymbol{z}_{D} \\right\\|}_{2} = 1$\\\n",
        "in contradiction.\\\n",
        "So the octahedron is a simple example of a 3D graph that cannot be embedded in any dimension while conserving all distances on the graph.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDd4prRjbPH0"
      },
      "source": [
        "### 4.2. Question\n",
        "\n",
        " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ be the training set.\n",
        " * Let $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$ be the representation obtained by IsoMap (Encoded data).\n",
        " * Consider a new point $\\boldsymbol{x}^{\\ast}$ where $\\boldsymbol{x}^{\\ast} = \\boldsymbol{x}_{k}, \\; k \\in \\left\\{ 1, 2, \\ldots, N \\right\\}$.\n",
        " * Let $\\boldsymbol{z}^{\\ast}$ be the out of sample encoding applied to $\\boldsymbol{x}^{\\ast}$.\n",
        "\n",
        "Prove or disprove: $\\boldsymbol{z}^{\\ast} = \\boldsymbol{z}_{k}$.\n",
        "\n",
        " * <font color='brown'>(**#**)</font> The out of sample encoding is as shown in _lecture notes_.\n",
        " * <font color='brown'>(**#**)</font> The question is basically if the out of sample extension is equivalent to having the point in the training set for such case.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL7GVNtsbPH0"
      },
      "source": [
        "### 4.2. Solution\n",
        "\n",
        "We obtain the encoding of the training set by:\\\n",
        "$\\boldsymbol{Z} = \\boldsymbol{\\Sigma}_d\\boldsymbol{V}^T_d$\\\n",
        "We can write the $k^{th}$ element as:\\\n",
        "$\\boldsymbol{z}_k = \\boldsymbol{\\Sigma}_d\\boldsymbol{V}^T_d \\hat{\\boldsymbol{e}}_k$\\\n",
        "where $\\hat{\\boldsymbol{e}}_k$ is the one-hot vector with $1$ in the $k^{th}$ place and $0$ elsewhere.\n",
        "\n",
        "\n",
        "When we treat $\\boldsymbol{x}^{\\ast} = \\boldsymbol{x}_k$ as a point out of sample, we must first connect it to the graph and calculate distances to all points in the training set. Obviously, the connection point will be $\\boldsymbol{x}_k$. The distance to $\\boldsymbol{x}_k$ is $0$, same as the self-distance and all distances to other points are equal to their distance to $\\boldsymbol{x}_k$.\\\n",
        "The distances calculation would yield:\n",
        "\n",
        "$\\boldsymbol{D}_{xy} \\left[ i\\right] = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}^{\\ast} \\right\\|}^2_{2} =  {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_k \\right\\|}^2_{2} \\implies \\boldsymbol{D}_{xy} = \\begin{bmatrix} \\left\\| \\boldsymbol{x}_{1} - \\boldsymbol{x}_k \\right\\|_{2}^{2} \\\\ \\left\\| \\boldsymbol{x}_{2} - \\boldsymbol{x}_k \\right\\|_{2}^{2} \\\\ \\vdots \\\\ 0 \\text{ (i=k)}\\\\ \\vdots\\\\ \\left\\| \\boldsymbol{x}_{N} - \\boldsymbol{x}_k \\right\\|_{2}^{N} \\end{bmatrix}$\\\n",
        "In fact, $\\boldsymbol{D}_{xy}$ is a column of $\\boldsymbol{D}_{xx}$ and we can write:\\\n",
        "$\\boldsymbol{D}_{xy} = \\boldsymbol{D}_{xx}\\hat{\\boldsymbol{e}}_k$\n",
        "\n",
        "Recall:\\\n",
        "$\\tilde{\\boldsymbol{K}}_{xx} = -\\frac 1 2\\boldsymbol{J}(\\boldsymbol{D}_{xx} - \\frac {1} {N_{x}}\\boldsymbol{D}_{xx}\\boldsymbol{1}_{N_{x}}\\boldsymbol{1}_{N_{x}}^T) \\in \\mathbb{R}^{N \\times N}$\\\n",
        "$\\tilde{\\boldsymbol{K}}_{xy} = -\\frac 1 2\\boldsymbol{J}(\\boldsymbol{D}_{xy} - \\frac {1} {N_{x}}\\boldsymbol{D}_{xx}\\boldsymbol{1}_{N_{x}}\\boldsymbol{1}_{N_{y}}^T) \\in \\mathbb{R}^{N \\times 1}$\\\n",
        "Note that given $N_{y} = 1$, $\\boldsymbol{1}_{N_{y}}^T = 1$\\\n",
        "Also, $\\boldsymbol{1}_{N_{x}}^T\\hat{\\boldsymbol{e}}_k = \\boldsymbol{1}_{N_{y}}^T= 1$\\\n",
        "Therefore, we have:\\\n",
        "$\\tilde{\\boldsymbol{K}}_{xy} = -\\frac 1 2\\boldsymbol{J}(\\boldsymbol{D}_{xx}\\hat{\\boldsymbol{e}}_k - \\frac {1} {N_{x}}\\boldsymbol{D}_{xx}\\boldsymbol{1}_{N_{x}}\\boldsymbol{1}_{N_{x}}^T\\hat{\\boldsymbol{e}}_k) = -\\frac 1 2\\boldsymbol{J}(\\boldsymbol{D}_{xx} - \\frac {1} {N_{x}}\\boldsymbol{D}_{xx}\\boldsymbol{1}_{N_{x}}\\boldsymbol{1}_{N_{x}}^T)\\hat{\\boldsymbol{e}}_k = \\tilde{\\boldsymbol{K}}_{xx}\\hat{\\boldsymbol{e}}_k$\n",
        "\n",
        "The out of sample extension is:\\\n",
        "$\\boldsymbol{z}^* = \\boldsymbol{\\Sigma}_d^{-1}\\boldsymbol{V}_d^T\\tilde{\\boldsymbol{K}}_{xy} = \\boldsymbol{\\Sigma}_d^{-1}\\boldsymbol{V}_d^T\\tilde{\\boldsymbol{K}}_{xx}\\hat{\\boldsymbol{e}}_k$\n",
        "\n",
        "\n",
        "If we used the original decomposition:\\\n",
        "$\\tilde{\\boldsymbol{K}}_{xx} = \\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^T$, we get:\\\n",
        "$\\boldsymbol{z}^* =  \\boldsymbol{\\Sigma}_d^{-1}\\boldsymbol{V}_d^T\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^T\\hat{\\boldsymbol{e}}_k = \\boldsymbol{\\Sigma}_d\\boldsymbol{V}^T_d \\hat{\\boldsymbol{e}}_k = \\boldsymbol{z}_k $\n",
        "\n",
        "*In the expression above $\\boldsymbol{V}_d^T\\boldsymbol{V}$ produce the $0$ columns that reduce all dimensions to $d$.\n",
        "\n",
        "We then showed that $\\boldsymbol{z}^* = \\boldsymbol{z}_k $\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ure0E1PLbPH0"
      },
      "source": [
        "## 5. Laplacian Eigenmaps\n",
        "\n",
        " * Let $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
        " * Let $G = \\left( V, E, W \\right)$ be a weighted graph with with $V = \\mathcal{X}$.\n",
        " * Define $\\boldsymbol{W} \\left[ i, j \\right] = \\begin{cases} \\exp \\left( - \\frac{ {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}_{2}^{2} }{2 {\\sigma}^{2}} \\right) & \\text{ if } \\boldsymbol{x}_{i} \\in \\mathcal{N}_{j} \\text{ or } \\boldsymbol{x}_{j} \\in \\mathcal{N}_{i} \\\\ 0 & \\text{ else } \\end{cases}$.\n",
        " * Then ${e}_{ij} \\in E$ if $\\boldsymbol{W} \\left[ i, j \\right] \\neq 0$.\n",
        " * The _Graph Laplacian_ $\\boldsymbol{L} = \\boldsymbol{D} - \\boldsymbol{W}$.\n",
        " * The _Degree Matrix_ $\\boldsymbol{D} = \\operatorname{Diag} \\left( \\boldsymbol{W} \\boldsymbol{1} \\right)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arVP9E0dbPH0"
      },
      "source": [
        "Assume that $G$ has 2 connected components, that is $V = {V}_{1} \\cup {V}_{2}$ such that $\\left\\{ {e}_{ij} \\mid i \\in {V}_{1}, j \\in {V}_{2} \\right\\} = \\emptyset$.\n",
        "\n",
        "### 5.2. Question\n",
        "\n",
        "Show that the _Graph Laplacian_ $\\boldsymbol{L}$ has two **orthogonal** eigenvectors corresponding to the zero eigenvalue.  \n",
        "That is, there are $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\in \\mathbb{R}^{N}$ such that:\n",
        "\n",
        " * $\\boldsymbol{L} \\boldsymbol{u}_{1} = \\boldsymbol{L} \\boldsymbol{u}_{2} = \\boldsymbol{0}$.\n",
        " * $\\left \\langle \\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\right \\rangle = 0$.\n",
        "\n",
        "Explain the meaning of the result, specifically address:\n",
        "\n",
        " * How to do spectral clustering in such case.\n",
        " * How to do dimensionality reduction in such case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFJNzfnhbPH1"
      },
      "source": [
        "### 5.2. Solution\n",
        "\n",
        "In the case of 2 connected components in the graph, we can assign the sample indices to being either in $V_1$ or in $V_2$. The intersection is empty: ${V}_{1} \\cap {V}_{2} = \\emptyset$\n",
        "\n",
        "The construction of $\\boldsymbol{L}$ is as usual, therefore we know that it has an eigenvector $\\boldsymbol{1}$ with eigenvalue $0$.\\\n",
        "$\\implies \\boldsymbol{u}_{1} = \\boldsymbol{1}, \\quad \\boldsymbol{L} \\boldsymbol{u}_{1} =0$\n",
        "\n",
        "In addition we will show that the vector $\\boldsymbol{f}$ which served as our target for the un-relaxed problem, with weights defined by $\\left|V_1\\right|, \\left|V_2\\right|$, is also a eigenvector of eigenvalue $0$. We define $\\boldsymbol{f}$ as follows:\\\n",
        "$\\boldsymbol{f}_i = \\begin{cases} \\sqrt{\\frac {\\left|V_2\\right|}{\\left|V_1\\right|}} & i \\in V_1 \\\\ -\\sqrt{\\frac {\\left|V_1\\right|}{\\left|V_2\\right|}} & i \\in V_2 \\end{cases}$\\\n",
        "We will show that $\\boldsymbol{L} \\boldsymbol{f} =0$ by showing each element is $0$:\n",
        "\n",
        "\n",
        "$$(\\boldsymbol{L} \\boldsymbol{f})_i = \\sum_j \\boldsymbol{L}_{ij} \\boldsymbol{f}_j \\stackrel{{V}_{1} \\cap {V}_{2} = \\emptyset}{=} \\sum_{j \\in V_1} \\boldsymbol{L}_{ij} \\boldsymbol{f}_{j} + \\sum_{j \\in V_2} \\boldsymbol{L}_{ij} \\boldsymbol{f}_j = \\sqrt{\\frac {\\left|V_2\\right|}{\\left|V_1\\right|}}\\sum_{j \\in V_1} \\boldsymbol{L}_{ij}  -\\sqrt{\\frac {\\left|V_1\\right|}{\\left|V_2\\right|}} \\sum_{j \\in V_2} \\boldsymbol{L}_{ij} $$\n",
        "\n",
        "For $i \\in V_1$ we have:\n",
        "* $\\left\\{ {e}_{ij} \\mid i \\in {V}_{1}, j \\in {V}_{2} \\right\\} = \\emptyset \\implies \\boldsymbol{L}_{ij} = 0 \\quad \\forall j\\in V_2 \\implies \\sum_{j \\in V_2} \\boldsymbol{L}_{ij} = 0$\n",
        "* $\\sum_{j \\in V_1} \\boldsymbol{L}_{ij} = \\boldsymbol{L}_{ii} + \\sum_{j_{\\neq i} \\in V_1} \\boldsymbol{L}_{ij} = \\sum_{j \\in V_1} \\boldsymbol{W}_{ij} - \\sum_{j \\in V_1} \\boldsymbol{W}_{ij} = 0$\n",
        "\n",
        "$\\implies (\\boldsymbol{L} \\boldsymbol{f})_i = 0$\\\n",
        "The symmetric argument is true for $i \\in V_2$\n",
        "\n",
        "$\\implies \\boldsymbol{L} \\boldsymbol{f} = 0$\n",
        "\n",
        "In addition, we have:\\\n",
        "$\\langle \\boldsymbol{f}, \\boldsymbol{1} \\rangle = \\sum_i \\boldsymbol{f}_i = \\left|V_1\\right|\\sqrt{\\frac {\\left|V_2\\right|}{\\left|V_1\\right|}} - \\left|V_2\\right|\\sqrt{\\frac {\\left|V_1\\right|}{\\left|V_2\\right|}} = \\sqrt{\\left|V_1\\right|\\left|V_2\\right|} - \\sqrt{\\left|V_2\\right|\\left|V_1\\right|} = 0$\n",
        "\n",
        "\n",
        "[ also, $\\| \\boldsymbol{f}\\|^2 = \\sum_i \\boldsymbol{f}_i^2 = \\left|V_1\\right|\\frac {\\left|V_2\\right|}{\\left|V_1\\right|} + \\left|V_2\\right|\\frac {\\left|V_1\\right|}{\\left|V_2\\right|} = {\\left|V_2\\right|}+{\\left|V_1\\right|} = N \\quad$ (although we do not use it here)]\n",
        "\n",
        "So $\\boldsymbol{u}_2 = \\boldsymbol{f}$ satisfies:\n",
        " * $\\boldsymbol{L} \\boldsymbol{u}_{2} = \\boldsymbol{0}$.\n",
        " * $\\left \\langle \\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\right \\rangle = 0$.\n",
        "\n",
        "The $\\boldsymbol{u}_{2}$ solution represents the cut between the two components. The cost of the cut is $0$ since they are disconnected and indeed our Ratio-Cut cost function is at its minimum with $\\boldsymbol{u}_{2}^T\\boldsymbol{L} \\boldsymbol{u}_{2} = \\boldsymbol{0}$.\\\n",
        "In fact, if we had more connected components, we would have additional eigenvectors with $0$ eigenvalue. The 0 degeneracy is the same as the number of independent components.\\\n",
        "Clustering into 2 clusters is easy using $\\boldsymbol{u}_{2}$. When we find its values, we can assign the samples to clusters according to the sign in their index, e.g. positive valued indices $\\in V_1$ and negative valued incdices $\\in V_2$.\\\n",
        "$V_1 = \\{ i | {\\boldsymbol{u}_{2}}_{i} > 0 \\}$\\\n",
        "$V_2 = \\{ i | {\\boldsymbol{u}_{2}}_{i} < 0 \\}$\n",
        "\n",
        "**However**, the problem with this solution is that it does not take into account the sizes of the connected components. There could be only one sample disconnected from all else and this cut will separate only this sample. This happens since there are no connecting weights and so the cut-cost is 0 regardelss of the cluster sizes.\\\n",
        "This means that we must check clustering induces by $\\boldsymbol{u}_{2}$ etc. If the separation is not good, we can consider using the eigenvector with the next smallest eigenvalue (non-zero) and perform clustering.\n",
        "\n",
        "If we want more clusters, we can do several things.\n",
        "1. We can change the graph connectivity until we find the desired number of connected components by the number of 0 eigenvalues. We do not take the constant vector $\\boldsymbol{u}_{1} = \\boldsymbol{1}$ since this is meaningless (constant value in this dimension).\\\n",
        "We want to keep the next smallest $d$ eigenvactors and extract the representation from them.\n",
        "\n",
        "2. We can take the $d$ first non constant eigenvectors without changing connectivity and applying another clustering algorithm on the induced representation.\n",
        "\n",
        "3. We can work similarly but hierarchically within each cluster we found.\n",
        "\n",
        "In the dimensionality reduction we would like to take the $d$ most usefull eigenvectors corresponding to the smallest eigenvalues of $\\boldsymbol{L}$.\\\n",
        "$\\boldsymbol{L} = \\boldsymbol{V}^T\\boldsymbol{\\Lambda}\\boldsymbol{V}$\\\n",
        "$\\boldsymbol{Z} = \\boldsymbol{V}^T_{\\{d\\}}$ (The representation for sample $i$ is composed of the $i^{th}$ entries in each of the $d$ vectors.) \\\n",
        "Suppose $\\boldsymbol{v}_{1} = \\boldsymbol{u}_{1}, \\boldsymbol{v}_{2} = \\boldsymbol{u}_{2}$ (both 0 eigenvalue)\n",
        "\n",
        "We do not take the constant vector $\\boldsymbol{u}_{1} = \\boldsymbol{1}$ since this is meaningless (constant value in this dimension).\\\n",
        "We should examine the clustering induced by $\\boldsymbol{u}_{2}$ and see if we want to use it. If so, we keep it and use eigenvectors up to the $d+1$ smallest eigenvactors and extract the representation from them: $\\boldsymbol{u}_2, \\boldsymbol{v}_3, \\dots \\boldsymbol{v}_{d+1}$.\n",
        "\n",
        "If the cut induced by $\\boldsymbol{u}_2$ is meaningless, we can skip it as well and take eigenvectors $\\boldsymbol{v}_3, \\boldsymbol{v}_4, \\dots \\boldsymbol{v}_{d+2}$.\\\n",
        "This should be equivalent to applying Laplacian eigenmaps algorithm within each connected component separately.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6ZM8iMnbPH1"
      },
      "source": [
        "## 6. t-SNE (Bonus 4 Points)\n",
        "\n",
        "The t-SNE objective is given by:\n",
        "\n",
        "$$ \\arg \\min_{ \\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N} } f \\left( \\boldsymbol{Z} \\right) = \\arg \\min_{ \\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N} } {D}_{KL} \\left( \\boldsymbol{P} \\mid \\mid \\boldsymbol{Q} \\right) = \\arg \\min_{ \\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N} } \\sum_{i = 1}^{N} \\sum_{j = 1}^{N} {p}_{ij} \\log \\left( \\frac{{p}_{ij}}{{q}_{ij}} \\right) $$\n",
        "\n",
        "Look for the definitions of $\\boldsymbol{P}$ and $\\boldsymbol{Q}$ **in the context of t-SNE** in lecture notes.\n",
        "\n",
        "### 6.1. The t-SNE Objective Gradient\n",
        "\n",
        "In the following sub questions the gradient of the objective function $\\nabla f \\left( \\boldsymbol{Z} \\right)$ will be derived in multiple steps.\n",
        "\n",
        "#### 6.1.1. Question\n",
        "\n",
        "Show that $f \\left( \\boldsymbol{Z} \\right) = {D}_{KL} \\left( \\boldsymbol{P} \\mid \\mid \\boldsymbol{Q} \\right)$ can be written as $f \\left( \\boldsymbol{Z} \\right) = C - \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{Q} \\right) \\right \\rangle$ for some constant $c$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> The constant $C$ above is actually the _Entropy_ of $\\boldsymbol{P}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCSpn7jDbPH1"
      },
      "source": [
        "#### 6.1.1. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrxwSoU-bPH1"
      },
      "source": [
        " * Reminder $\\boldsymbol{Q} \\left[ i, j \\right] = \\frac{1}{B} \\begin{cases} 0 & \\text{ if } i = j \\\\ {\\left( 1 + {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}^{2} \\right)}^{-1} & \\text{ if } i \\neq j \\end{cases}$.\n",
        " * Let $\\boldsymbol{D}_{z} \\in \\mathbb{R}^{N \\times N}$ where $\\boldsymbol{D}_{z} \\left[ i, j \\right] = {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2}^{2}$.  \n",
        " * Let $\\boldsymbol{S} = {\\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right)}^{\\circ -1} \\in \\mathbb{R}^{N \\times N}$ that is $\\boldsymbol{S} \\left[ i, j \\right] = {\\left( 1 + \\boldsymbol{D}_{z} \\left[ i, j \\right] \\right)}^{-1}$.\n",
        "\n",
        "\n",
        "#### 6.1.2. Question\n",
        "\n",
        "Show that $B = \\boldsymbol{1}^{T} \\left( \\boldsymbol{S} - \\boldsymbol{I} \\right) \\boldsymbol{1} \\in \\mathbb{R}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t10JUH0ObPH1"
      },
      "source": [
        "#### 6.1.2. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7A2mxtRbPH1"
      },
      "source": [
        "#### 6.1.3. Question\n",
        "\n",
        "Show that $\\boldsymbol{Q} = \\frac{1}{B} \\left( \\boldsymbol{S} - \\boldsymbol{I} \\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbxeYHE9bPH2"
      },
      "source": [
        "#### 6.1.3. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfN1GSerbPH2"
      },
      "source": [
        "#### 6.1.4. Question\n",
        "\n",
        "Show that $- \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{Q} \\right) \\right \\rangle = \\log \\left( B \\right) + \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right) \\right \\rangle$.\n",
        "\n",
        " * <font color='brown'>(**#**)</font> Think of the value of $\\boldsymbol{P} \\left[ i, i \\right]$ and $\\boldsymbol{1}^{T} \\boldsymbol{P} \\boldsymbol{1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGrtBsdybPH2"
      },
      "source": [
        "#### 6.1.4. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYXZU5NVbPH2"
      },
      "source": [
        "Let $f \\left( \\boldsymbol{Z} \\right) = C + \\log \\left( B \\right) + \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right) \\right \\rangle$\n",
        "\n",
        "#### 6.1.5. Question\n",
        "\n",
        "Show that ${\\nabla}_{z} \\left \\langle \\boldsymbol{P}, \\log \\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right) \\right \\rangle \\left[ \\boldsymbol{H} \\right] = \\left \\langle \\boldsymbol{S} \\circ \\boldsymbol{P}, {\\nabla}_{z} \\left[ \\boldsymbol{H} \\right] \\right \\rangle$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may use $\\nabla \\boldsymbol{S} \\left[ \\boldsymbol{H} \\right] = \\nabla {\\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right)}^{\\circ -1} \\left[ \\boldsymbol{H} \\right] = - {\\left( \\boldsymbol{1} \\boldsymbol{1}^{T} + \\boldsymbol{D}_{z} \\right)}^{\\circ -2} \\circ \\nabla \\left( \\boldsymbol{D}_{z} \\right) \\left[ \\boldsymbol{H} \\right] = - \\boldsymbol{S} \\circ \\boldsymbol{S} \\circ \\nabla \\left( \\boldsymbol{D}_{z} \\right) \\left[ \\boldsymbol{H} \\right]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44kDTl6lbPH2"
      },
      "source": [
        "#### 6.1.5. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35WsMhipbPH2"
      },
      "source": [
        "\n",
        "#### 6.1.6. Question\n",
        "\n",
        "Show that ${\\nabla}_{z} \\log \\left( B \\right) = \\left \\langle \\boldsymbol{S} \\circ \\boldsymbol{Q}, \\nabla \\left( \\boldsymbol{D}_{z} \\right) \\left[ \\boldsymbol{H} \\right] \\right \\rangle$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may use $\\boldsymbol{Q} = \\frac{1}{B} \\left( \\boldsymbol{S} - \\boldsymbol{I} \\right)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnWnUhkQbPH3"
      },
      "source": [
        "#### 6.1.6. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUCN-HUGbPH3"
      },
      "source": [
        "\n",
        "#### 6.1.7. Question\n",
        "\n",
        "Combine all previous and writhe the gradient of the objective $\\nabla f \\left( \\boldsymbol{Z} \\right)$.\n",
        "\n",
        "* <font color='brown'>(**#**)</font> You may use $\\boldsymbol{A} = \\left( \\boldsymbol{P} - \\boldsymbol{Q} \\right) \\circ \\boldsymbol{S}$ to simplify the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXDpj8MqbPH3"
      },
      "source": [
        "#### 6.1.7. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1zJzpwbPH3"
      },
      "source": [
        "\n",
        "#### 6.1.8. Question\n",
        "\n",
        "What can you say about the gradient of $\\nabla f \\left( \\boldsymbol{Z} \\right)$ when $\\boldsymbol{P} = \\boldsymbol{Q}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpcz9a2sbPH3"
      },
      "source": [
        "#### 6.1.8. Solution\n",
        "\n",
        "<font color='red'>??? Fill the answer here ???</font>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
