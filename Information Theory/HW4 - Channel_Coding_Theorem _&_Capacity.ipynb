{"cells":[{"cell_type":"markdown","id":"49eaf30b","metadata":{"id":"49eaf30b"},"source":["##### Introduction to Information Theory (Fall 2023/4)\n","\n","# Home Assignment 4 (with some solutions)\n","\n","#### Topics:\n","- Lossless compression\n","\n","#### Due: 20/2/2024 before the class\n","\n","#### Instructions:\n","- Write your names and date in the cell below.\n","- Submit a copy of this notebook with code filled in the relevant places as the solution to coding exercises.\n","- You are welcomed to ask for hints, clarifications, or report issues on **Piazza**.\n","- For theoretic exercises, you can either write your solution in the notebook using $\\LaTeX$ (recommended) or submit additional notes.\n","- Logarithm is in base $2$ unless stated otherwise.\n","- For chain of equalities or inequalities, make sure to explain every non-trivial transition."]},{"cell_type":"markdown","id":"c1137232","metadata":{"id":"c1137232"},"source":["**Date**:\n","\n","**Name1**: Guy Taggar\n","\n","**Student ID1**:206260762\n","\n","**Name2**: Itay Shapira\n","\n","**Student ID2**: 034250043"]},{"cell_type":"markdown","id":"77440392","metadata":{"id":"77440392"},"source":["### 1. Operational Capacity of a Noiseless Channel\n","1. Prove that for any channel $P_{X^n|Y^n}$ we have $C_{op} \\leq  \\log|\\mathcal X|$ without using the channel coding theorem (That is, directly from the definition.). Use the following steps:\n"," - Suppose that there exists a $(2^{nR},n)$-coding scheme with $R >\\log|\\mathcal X|$. Find the number of messages that cannot be uniquely mapped to a channel input.\n"," - Find the proportion of this number of messages out of the entire message space.\n"," - Argue that the average error in decoding is at least half this proportion.\n"," - Complete the proof.\n","\n","2. Consider a channel with identical input and output alphabets $\\mathcal X$. Assume that for some $m \\in \\mathbb N$ the channel is noiseless, i.e.\n","$$\n","P_{Y^m|X^m}(y^m|x^m) = \\begin{cases} 1 & x^m = y^m \\\\\n","0 & x^m \\neq y^m.\n","\\end{cases}\n","$$\n","Show that the operational capacity of this channel is $C_{op} := \\log |\\mathcal X|$ by proving an achievability and converse claims (you are not allowed to use the channel coding theorem).\n"]},{"cell_type":"markdown","id":"c6c6ecc7","metadata":{"id":"c6c6ecc7"},"source":["Answers:\n","\n","1.\n","\n","Proof by Contradiction:\n","\n","1. Suppose for contradiction a channel $P_{X^{n}\\mid Y^{n}}$ and a $(2^{nR} ,n)$-coding scheme with $R> \\log|\\mathcal X|$. This means there are $2^{nR}$ messages encoded into $|\\mathcal X|^n$ codewords of length $n$. Since $R> \\log|\\mathcal X|$, this implies $2^{nR} > |\\mathcal X|^n$. Thus, the number of messages that cannot be uniquely mapped to a channel input is: $2^{nR} - |\\mathcal X|^n$.\n","\n","2. The proportion of unmappable messages will be: $\\frac{(2^{nR} - |\\mathcal X|^n)}{2^{nR}} = 1 - \\frac{|\\mathcal X|^n}{2^{nR}}$.\n","\n","3. Average error bound: Suppose we decode a message based on the received channel output. Since some messages share codewords, an error occurs whenever any of the unmappable messages get decoded as one of the remaining messages. Thus, given a output, it may be decoded to a number of messages out of the $2^{nR}$. At the best case cenario any unmappable output can be decoded to only two input messages, the correct input or not, with at most 50% chance of success. Thus the average error is at least half of these occasions.\n","\n","4. Contradiction and conclusion: We assumed that $R> \\log|\\mathcal X|$, leading to a proportion of unmappable messages and a minimum average error $P_{err,avg} \\geq \\frac{Proportion}{2}$. However, this contradicts the definition of channel capacity $C_{op}$, which guarantees reliable transmission (arbitrarily small $P_{err, max}$) at rates below its value. The average error $ \\frac{1}{2} * (1 - \\frac{|\\mathcal X|^n}{2^{nR}})$ will to go $\\frac{1}{2}$ as $n\\to\\infty$, and as the $P_{err,max} >  P_{err,avg}$ by definition, $P_{err,max}$ will not go to zero as $n\\to\\infty$.\n","\n","Therefore, our initial assumption of $R> \\log|\\mathcal X|$ must be false. Consequently, for any channel and any coding scheme, we have:\n","\n","$C_{op} \\leq log|\\mathcal X|$"]},{"cell_type":"markdown","id":"e57a9a22","metadata":{"id":"e57a9a22"},"source":["2.\n","\n","Achievability:\n","\n","\n","Given the noiseless channel property, if $x^m = y^m$, then the transmission is error-free. Therefore, one can encode a message directly into a sequence of symbols from $|\\mathcal X|^m$ and expect the same sequence at the output.\n","\n","For a message set of size $M$, one can use $m$ channel uses to send any of the $|\\mathcal X|^m$ possible sequences. This allows us to represent M = $|\\mathcal X|^m$ unique messages. The rate $R$ is then given by:\n","\n","  $R= \\frac{\\log M}{m} = \\frac{\\log |\\mathcal X|^m}{m} = \\frac{m\\log |\\mathcal X|}{m} = \\log |\\mathcal X|$\n","\n","This rate is achievable because each use of the channel can transmit $\\log |\\mathcal X|$ bits of information without error, thanks to the channel's noiseless property.\n","\n","Since the channel is noiseless for sequences of length $m$ , $P_{Y^m|X^m}(y^m|x^m) = 1$ if $x^m = y^m$, meaning $P_{err} = 0$ for any message sent. This perfectly aligns with the requirement for arbitrarily low error probability.\n","\n","\n","Converse:\n","\n","let $R>\\log |\\mathcal X|$\n","\n","We proved in the first part that $C_{op} \\leq  \\log|\\mathcal X|$ and as $R$ needed to be smaller that $C_{op}$ to be achievable. Thus in case when, $R>\\log |\\mathcal X|$, it will not achievable.\n"]},{"cell_type":"markdown","id":"d8a5526f","metadata":{"id":"d8a5526f"},"source":["### 2. Channel Capacity\n","(Based on Exc. 7.6 in Thomas \\& Cover) Consider a 26-key typewriter.\n","1. If pushing a key results in printing the associated letter, what is the capacity $C$ in bits?\n","2. Now suppose that pushing a key results in printing the associated letter or the next letter in the alphabet with equal probability. That is, $A \\to B$, $B\\to C$,...,$Z \\to A$. What is the capacity?\n","3. What is the highest rate code with block length one $(n=1)$ that you can find that achieves *zero* probability of error for the channel in part (2)?\n","\n","*Hint for (2)*:\n","Show first that for a channel with transition matrix in which the rows are permutations of each other and the columns are permutations of each other, the capacity is\n","$$\n","C = \\log |\\mathcal Y| - H( \\text{row of transition matrix}).\n","$$\n","(such a channel is called *symmetric*)"]},{"cell_type":"markdown","id":"XmOzjXN0qrXO","metadata":{"id":"XmOzjXN0qrXO"},"source":["### Answers\n","1. The output is just the input. Hence:\n","$$C = \\max I(X:Y) = \\max H(X) = \\log_2(26)\\approx 4.7$$\n","2. Given a transition matrix $P_{Y|X}$ for a channel where the rows and columns are permutations of each other, let's denote the output alphabet as $\\mathcal{Y}$ and the input alphabet as $\\mathcal{X}$.\n","\n","The transition matrix $P_{Y|X}$ can be represented as follows:\n","\n","$$P_{Y|X} = \\begin{bmatrix} p(y_1|x_1) & p(y_2|x_1) & \\cdots & p(y_k|x_1) \\\\ p(y_1|x_2) & p(y_2|x_2) & \\cdots & p(y_k|x_2) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ p(y_1|x_m) & p(y_2|x_m) & \\cdots & p(y_k|x_m) \\end{bmatrix} \\$$\n","\n","where $|\\mathcal{X}| = m$ and $|\\mathcal{Y}| = k$.\n","\n","Now, let's consider the entropy of a row, denoted as $H(\\text{row of transition matrix})$. Without loss of generality, let's focus on the first row:\n","\n","$$H(\\text{row of transition matrix}) = -\\sum_{i=1}^{k} p(y_i|x_1) \\log_2(p(y_i|x_1))$$\n","\n","Since the rows are permutations of each other, the entropy will be the same for any row. Therefore, we can say that:\n","$$H(Y|X) = H(\\text{row of transition matrix})$$\n","\n","since the rows and columns of the transition matrix are permutations of each other, the marginal distributions are uniform. i.e\n","$$H(Y)=\\log_2|\\mathcal Y|$$\n","\n","It follows that the capacity $C$ for a symmetric channel is given by:\n","\n","$$C = \\log_2(|\\mathcal Y|) - H( \\text{row of transition matrix})$$\n","\n","Now, if pushing a key results in printing the associated letter or the next letter in the alphabet with equal probability, we have a symmetric channel. According to the hint:\n","$$C = \\log_2(|\\mathcal Y|) - H( \\text{row of transition matrix})$$\n","\n","Let us solve for $H(\\text{row of transition matrix})$ while choosing the second row.\n","\n","$$H(\\text{row of transition matrix})=-\\sum_{i=1}^{26}P(i,1)\\log_2P(i,1)$$\n","Since $P(1,2)=P(2,2)=\\frac{1}{2}$ and $P(i,2)=0$ for $i>2$, we get that:\n","$$-\\sum_{i=1}^{26}P(i,1)\\log_2P(i,1)=-2\\cdot \\frac{1}{2}\\log_2\\frac{1}{2} = 1$$\n","\n","We conclude that:\n","$$C = \\log_2 26 - 1 \\approx 3.7$$\n","\n","3. The code should use every second codeword. This way, no conflicts will occur.\n","$$R=\\frac{\\log(\\frac{26}{2})}{n}\\approx3.7$$\n"]},{"cell_type":"markdown","id":"K-7JX4uclU5M","metadata":{"id":"K-7JX4uclU5M"},"source":["### Channel Capacity and Random Codes\n","(based on Exc. 7.8 and 7.9 in Thomas \\& Cover)\n","The $Z$-channel has a binary input and output alphabets and transition probabilities $P_{Y|X}$ given by the matrix\n","$$\n","P_{Y|X} = \\begin{bmatrix} 1 & 0 \\\\\n","1/2 & 1/2\n","\\end{bmatrix}\n","$$\n","Namely, $\\Pr[Y=0|X=0]=1$ while $\\Pr[Y=0|X=1]=1/2$ ($0$ goes noiselessly, while a $1$ may turn into a zero with probability $1/2$).\n","1. Find the capacity of this channel and the maximizing input probability distribution. It may help to know that\n","$$\n","\\frac{d}{dp} h_2(p) = \\log_2((1-p)/p).\n","$$\n","\n","2. Assume that we draw a random $(2^{nR},n)$ code (as in the proof of the channel coding theorem) for this channel in which each codeword is a sequence of *fair* coin tosses (this may not achieve capacity). Find the maximum rate $R$ such that the probability of error $P_{\\mathrm{err}}^{(n)}$ averaged over the randomly generated codes, tends to zero as $n$ tends to infinity.\n","\n"]},{"cell_type":"markdown","id":"2eb34b89","metadata":{"id":"2eb34b89"},"source":["Answers:\n","\n","1.\n","\n","As we studied in class, the channel capacity is defined as:\n","$$C=\\max_{P_X}I(X;Y)$$\n","Now let us find both $H(Y)$ and $H(Y|X)$ while setting $P_X(0)=p$\n","\n","$$P_Y(0) = P_X(0) + \\frac{1}{2}P_X(1)=\\frac{1+p}{2}$$\n","$$P_Y(1) = \\frac{1}{2}P_X(1)=\\frac{1-p}{2}$$\n","\n","Hence:\n","$$H(Y)=-\\frac{1+p}{2}\\log_2(\\frac{1+p}{2}) - \\frac{1-p}{2}\\log_2(\\frac{1-p}{2})$$\n","\n","Now let us calculate $H(Y|X)$.\n","$$H(Y|X) = H(Y|X) = -\\sum_{x,y} P_{XY}(x,y) \\log_2 P_{Y|X}(y|x) = $$\n","$$- P_{XY}(0,0)  \\cdot \\log_2 P_{Y|X}(0|0) - P_{XY}(0,1) \\cdot \\log_2 P_{Y|X}(1|0) - P_{XY}(1,0) \\cdot \\log_2 P_{Y|X}(0|1) - P_{XY}(1,1) \\cdot \\log_2 P_{Y|X}(1|1)= $$\n","$$- P_X(0) \\cdot P_{Y|X}(0|0) \\cdot \\log_2 P_{Y|X}(0|0) - P_X(0) \\cdot P_{Y|X}(1|0) \\cdot \\log_2 P_{Y|X}(1|0) - P_X(1) \\cdot P_{Y|X}(0|1) \\cdot \\log_2 P_{Y|X}(0|1) - P_X(1) \\cdot P_{Y|X}(1|1) \\cdot \\log_2 P_{Y|X}(1|1) = $$\n","\n","$$-p \\cdot 0 - p \\cdot 0 \\cdot 1 - \\frac{(1-p)}{2} \\cdot (-1) - \\frac{(1-p)}{2} \\cdot (-1) = 1-p$$\n","\n","Thus\n","$$I(X;Y) = -\\frac{1+p}{2}\\log_2(\\frac{1+p}{2}) - \\frac{1-p}{2}\\log_2(\\frac{1-p}{2}) + p - 1$$\n","Let $t=\\frac{1+p}{2}$. We get:\n","$$I(X;Y)= h_2(t) + 2t-2 $$\n","$$\\frac{d}{dt}I(X;Y)= \\log_2(\\frac{1-t}{t}) + 2 = 0 \\Longrightarrow$$\n","$$\\log_2(\\frac{1-t}{t}) = -2 \\Longrightarrow$$\n","$$\\frac{1-t}{t} = \\frac{1}{4} \\Rightarrow t=\\frac{4}{5}\\Rightarrow$$\n","$$P_X(0)=\\frac{3}{5}$$\n","\n","We now calculate the channel capacity\n","\n","$$C=-\\frac{4}{5}\\log_2\\frac{4}{5} - \\frac{1}{5}\\log_2\\frac{1}{5} + \\frac{3}{5} - 1 \\approx 0.322$$\n","\n","2.\n","\n","Let's compute the distribution of $Y$:\n","\n","$P(y=1)=P(y=1|x=0)P(x=0)+P(y=1|x=1)P(x=1)$\n","\n","$P(y=1)=0.5(0)+0.5(0.5)=0.25$\n","\n","$P(y=0)=P(y=0|x=0)P(x=0)+P(y=0|x=1)P(x=1)$\n","\n","$P(y=0)=0.5(1)+0.5(0.5)=0.75$\n","\n","$\\Rightarrow P_{Y}=\\left\\{ 0.75,0.25\\right\\} $\n","\n","$H(Y)=h_{2}(0.25)$\n","\n","$H(Y|X)=0.5*h_2(0.5)=0.5$  (same as above, $p_1=0.5$)\n","\n","$I(X;Y)=H(Y)-H(Y|X)$\n","\n","$I(X;Y) = h_{2}(0.25)-0.5 \\backsimeq 0.3113$\n","\n","According to the channel coding theorem, we can send information at a this rate corresponding with an arbitrarily\n","low probability of error.\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1nSQ6Fw-IsP1hH5wlNqXQGQBpqBIcNT1z","timestamp":1708267397967}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":5}
